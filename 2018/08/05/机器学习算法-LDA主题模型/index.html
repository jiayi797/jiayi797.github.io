<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
<!-- Referrer Policy调整致不蒜子单页面统计出错:https://senorui.top/posts/c33f.html -->
<meta name="referrer" content="no-referrer-when-downgrade">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiayi797.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="基本概览 概率主题模型：隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。 按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法-LDA主题模型">
<meta property="og:url" content="http://jiayi797.github.io/about/2018/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="甲乙小朋友的房子">
<meta property="og:description" content="基本概览 概率主题模型：隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。 按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-13-45-51.png">
<meta property="og:image" content="c:/Users/jiayi/AppData/Local/Temp/1533450274330.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-24-57.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-31-04.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-39-44.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-50-02.png">
<meta property="article:published_time" content="2018-08-05T05:18:30.000Z">
<meta property="article:modified_time" content="2018-12-17T07:42:01.000Z">
<meta property="article:author" content="jiayi797">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-13-45-51.png">

<link rel="canonical" href="http://jiayi797.github.io/about/2018/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习算法-LDA主题模型 | 甲乙小朋友的房子</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">甲乙小朋友的房子</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">甲乙小朋友很笨，但甲乙小朋友不会放弃</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jiayi797.github.io/about/2018/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-LDA%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jiayi797">
      <meta itemprop="description" content=".">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="甲乙小朋友的房子">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习算法-LDA主题模型
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-08-05 13:18:30" itemprop="dateCreated datePublished" datetime="2018-08-05T13:18:30+08:00">2018-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-12-17 15:42:01" itemprop="dateModified" datetime="2018-12-17T15:42:01+08:00">2018-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">机器学习算法</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="基本概览">基本概览</h1>
<p>概率主题模型：隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA）。</p>
<p>按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种<a target="_blank" rel="noopener" href="http://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">主题模型</a>，它可以<strong>将文档集中每篇文档的主题以概率分布的形式给出</strong>，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。</p>
<p> LDA模型有一个前提，就是一篇<strong>文档生成</strong>的方式（注意是文档哦）如下：</p>
<ul>
<li>从狄利克雷分布<span class="math inline">\(\alpha\)</span>中取样生成文档i的主题分布<span class="math inline">\(\theta_i\)</span></li>
<li>从文档i的主题分布<span class="math inline">\(\theta_i\)</span>中取样生成文档i第j个词的主题<span class="math inline">\(z_{i,j}\)</span></li>
<li>从狄利克雷分布<span class="math inline">\(\beta\)</span>中取样生成主题<span class="math inline">\(z_{i,j}\)</span>对应的词语分布<span class="math inline">\(\phi_{z_{i,j}}\)</span></li>
<li>从词语的多项式分布<span class="math inline">\(\phi_{z_{i,j}}\)</span>中采样最终生成词语<span class="math inline">\(w_{i,j}\)</span></li>
</ul>
<span id="more"></span>
<p>其中，类似Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布。</p>
<p> 此外，LDA的图模型结构如下图所示（类似<a target="_blank" rel="noopener" href="http://blog.csdn.net/v_july_v/article/details/40984699#t6">贝叶斯网络</a>结构）：</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-13-45-51.png" /></p>
<p>知道理解LDA，可以分为下述5个步骤：</p>
<ol type="1">
<li>一个函数：gamma函数</li>
<li>四个分布：二项分布、多项分布、beta分布、Dirichlet分布</li>
<li>一个概念和一个理念：共轭先验和贝叶斯框架</li>
<li>两个模型：pLSA、LDA</li>
<li>一个采样：Gibbs采样</li>
</ol>
<p>本文便按照上述5个步骤来阐述。</p>
<h1 id="几个分布">几个分布</h1>
<h2 id="二项分布">二项分布</h2>
<p>二项分布是从伯努利分布推进的。伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负{+，-}。而二项分布即重复n次的伯努利试验，记为<span class="math inline">\(X\text{~}b(n,p)\)</span>。简言之，只做一次实验，是伯努利分布，重复做了n次，是二项分布。二项分布的概率密度函数为：</p>
<p><span class="math display">\[P(K=k)= \begin{pmatrix} n \\ k \\ \end{pmatrix} p^k(1-p)^{n-k} \]</span></p>
对于k=0,1,2,....,n，其中的$
<span class="math display">\[\begin{pmatrix} n \\ k \\ \end{pmatrix}\]</span>
<p>=<span class="math inline">\(是二项式系数，又记作\)</span>C(n,k)$。</p>
<h2 id="多项分布"><strong>多项分布</strong></h2>
<p>多项分布，是二项分布扩展到多维的情况</p>
<p>多项分布是指单次试验中的随机变量的取值不再是0-1的，而是有多种离散值可能（1,2,3...,k）。比如投掷6个面的骰子实验，N次实验结果服从K=6的多项分布。其中</p>
<p><span class="math display">\[\sum_{i=1}^kp_i=1, p_i &gt; 0\]</span></p>
<p>多项分布的概率密度函数为：</p>
<p><span class="math display">\[P(x_1,x_2,...,x_k;p_1,p_2,...,p_k)=\frac{n!}{x_1!...x_k!}p_1^{x^1}...p_k^{x_k}\]</span></p>
<h2 id="beta分布">Beta分布</h2>
<p>Beta分布是二项分布的共轭先验分布</p>
<p>给定参数<span class="math inline">\(\alpha&gt;0, \beta &gt; 0\)</span>，取值范围为[0,1]的随机变量x的概率密度函数：</p>
<p><span class="math display">\[f(x;\alpha, \beta)=\frac{1}{B(\alpha, \beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span></p>
<p>其中，</p>
<p><span class="math display">\[\frac{1}{B(\alpha,\beta)}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}, \Gamma(z)=\int_0^{\infty} {t^{-1}e^{-t}} \,{\rm d}t  \]</span></p>
<p>其中，<span class="math inline">\(\Gamma(x)\)</span>是gamma函数。</p>
<h2 id="dirichlet分布"><strong>Dirichlet分布</strong></h2>
<p>Dirichlet分布，是beta分布在高维度上的推广</p>
<p>Dirichlet分布的的密度函数形式跟beta分布的密度函数如出一辙 ：</p>
<p><span class="math display">\[{\displaystyle f(x_{1},\dots ,x_{K};\alpha _{1},\dots ,\alpha _{K})={\frac {1}{\mathrm {B} (\alpha )}}\prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}}\]</span></p>
<p>其中，<span class="math inline">\({\mathrm {B}}(\alpha )={\frac {\prod _{i=1}^{K}\Gamma (\alpha _{i})}{\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}},\qquad \alpha =(\alpha _{1},\dots ,\alpha _{K}).\)</span></p>
<p>至此，我们可以看二项分布和多项分布很相似，Beta分布和Dirichlet 分布很相似，而至于“Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布”。</p>
<h2 id="总结">总结</h2>
<p>我们知道beta分布是二项式分布的共轭先验概率分布。对于非负实数<span class="math inline">\(\alpha,\beta\)</span>，我们有如下关系：</p>
<p><span class="math display">\[Beta(p|\alpha,\beta)=Count(m_1,m_2)=Beta(p|\alpha+m_1, \beta+m_2)\]</span></p>
<p>其中<span class="math inline">\((m_1,m_2)\)</span>对应的是二项分布<span class="math inline">\(B(m_1+m_2,p)\)</span>的计数。针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial 共轭。</p>
<p>我们知道狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布，把<span class="math inline">\(\alpha\)</span>从整数集合拓展到实数集合，从而得到更一般的表达式：</p>
<p><span class="math display">\[Dir(\vec{p}|\vec{\alpha})=MultCount(\vec{m})=Beta(p|\vec{\alpha}+\vec{m})\]</span></p>
<p> 针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是 Dirichlet-Multinomial 共轭。</p>
<h1 id="主题模型lda">主题模型LDA</h1>
<p>在杀到终极boss——LDA模型之前，再循序渐进理解基础模型：Unigram model、mixture of unigrams model，以及跟LDA最为接近的pLSA模型。为了方便描述，首先定义一些变量：</p>
<ul>
<li><span class="math inline">\(w\)</span>表示词，<span class="math inline">\(V\)</span>表示所有单词个数</li>
<li><span class="math inline">\(z\)</span>表示主题，<span class="math inline">\(k\)</span>是主题个数</li>
<li><span class="math inline">\(D=(w_1,...,w_M)\)</span>表示语料库，M是文档数目</li>
<li><span class="math inline">\(w=(w_1,...,w_N)\)</span>表示文档，N是文档中的词数</li>
</ul>
<h2 id="unigram-model">Unigram model</h2>
<p>对于文档<span class="math inline">\(w=(w_1,...,w_N)\)</span>，用<span class="math inline">\(p(w_n)\)</span>表示词<span class="math inline">\(w_n\)</span>的先验概率，生成文档w的概率为：</p>
<p><span class="math display">\[p(w)=\prod_{n=1}^Np(w_n)\]</span></p>
<p>其图模型为：</p>
<p>图中被涂色的w表示可观测变量，N表示一篇文档中总共N个单词，M表示M篇文档</p>
<figure>
<img src="C:\Users\jiayi\AppData\Local\Temp\1533450274330.png" alt="1533450274330" /><figcaption aria-hidden="true">1533450274330</figcaption>
</figure>
<p>或为：</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-24-57.png" /></p>
<p>unigram model假设文本中的词服从Multinomial分布，而我们已经知道Multinomial分布的先验分布为Dirichlet分布。</p>
<p>上图中的<span class="math inline">\(w_n\)</span>表示在文本中观察到的第n个词，<span class="math inline">\(n\in[1,N]\)</span>表示该文本中一共有N个单词。加上方框表示重复，即一共有N个这样的随机变量<span class="math inline">\(w_n\)</span>。其中，p和<span class="math inline">\(\alpha\)</span>是隐含未知变量。</p>
<ul>
<li>p是词服从的Multinomial分布的参数</li>
<li>α是Dirichlet分布（即Multinomial分布的先验分布）的参数。</li>
</ul>
<p>一般α由经验事先给定，p由观察到的文本中出现的词学习得到，表示文本中出现每个词的概率。</p>
<h2 id="plsa模型">PLSA模型</h2>
<p>因为跟LDA模型最为接近的便是下面要阐述的这个pLSA模型，理解了pLSA模型后，到LDA模型也就一步之遥——给pLSA加上贝叶斯框架，便是LDA。</p>
<h3 id="根据主题词生成文档">根据主题词生成文档</h3>
<p>实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。比如介绍一个国家的文档中，往往会分别从教育、经济、交通等多个主题进行介绍。那么在pLSA中，<strong>文档是怎样被生成的呢</strong>？</p>
<p>假设你要写M篇文档，由于一篇文档由各个不同的词组成，所以你需要确定每篇文档里每个位置上的词。</p>
<p>再假定你一共有K个可选的主题，有V个可选的词。一般来说，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。  最后，你不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复N次（产生N个词），完成一篇文档，重复这产生一篇文档的方法M次，则完成M篇文档。</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-31-04.png" /></p>
<p>上述过程抽象出来即是PLSA的文档生成模型。在这个过程中，我们并未关注词和词之间的出现顺序，所以pLSA是一种词袋方法。具体说来，该模型假设一组共现(co-occurrence)词项关联着一个隐含的主题类别<span class="math inline">\(z_k\in{z_1,...,z_K}\)</span>。同时定义：</p>
<ul>
<li><span class="math inline">\(P(d_i)\)</span>表示海量文档中某篇文档被选中的概率；</li>
<li><span class="math inline">\(P(w_j|d_i)\)</span>表示词<span class="math inline">\(w_j\)</span>在给定文档<span class="math inline">\(d_i\)</span>中出现的概率；</li>
<li><span class="math inline">\(P(z_k|d_i)\)</span>表示具体某个主题<span class="math inline">\(z_k\)</span>在给定文档<span class="math inline">\(d_i\)</span>下出现的概率</li>
<li><span class="math inline">\(P(w_j|z_k)\)</span>表示具体某个词<span class="math inline">\(w_j\)</span>在给定主题<span class="math inline">\(z_k\)</span>下出现的概率。与主题关系越密切的词，其条件概率越大。</li>
</ul>
<p> 利用上述的第1、3、4个概率，我们便可以按照如下的步骤得到“文档-词项”的生成模型：</p>
<ol type="1">
<li>按照概率<span class="math inline">\(P(d_i)\)</span>选择一篇文档<span class="math inline">\(d_i\)</span></li>
<li>选定文档<span class="math inline">\(d_i\)</span>后，从主题分布中按照概率<span class="math inline">\(P(z_k|d_i)\)</span>选择一个隐含的主题类别<span class="math inline">\(z_k\)</span></li>
<li>选定<span class="math inline">\(z_k\)</span>后，从词分布中按照概率<span class="math inline">\(P(w_j|z_k)\)</span>选一个词<span class="math inline">\(w_j\)</span></li>
</ol>
<p>所以pLSA中生成文档的整个过程便是选定文档生成主题，确定主题生成词。</p>
<h3 id="根据文档反推主题分布">根据文档反推主题分布</h3>
<p>反过来，既然文档已经产生，那么如何根据已经产生好的文档反推其主题呢？这个利用看到的文档推断其隐藏的主题（分布）的过程（其实也就是产生文档的逆过程），便是主题建模的目的：自动地发现文档集中的主题（分布）。</p>
<p>换言之，人类根据文档生成模型写成了各类文章，然后丢给了计算机，相当于计算机看到的是一篇篇已经写好的文章。现在计算机需要根据一篇篇文章中看到的一系列词归纳出当篇文章的主题，进而得出各个主题各自不同的出现概率：主题分布。即文档d和单词w是可被观察到的，但主题z却是隐藏的。</p>
<p>如下图所示（图中被涂色的d、w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-39-44.png" /></p>
<p>上图中，文档d和词w是我们得到的样本（样本随机，参数虽未知但固定，所以pLSA属于频率派思想。区别于下文要介绍的LDA中：样本固定，参数未知但不固定，是个随机变量，服从一定的分布，所以LDA属于贝叶斯派思想），可观测得到，所以对于任意一篇文档，其<span class="math inline">\(P(z_k|d_i)\)</span>是已知的。</p>
<p> 从而可以根据大量已知的文档-词项信息 <span class="math inline">\(P(w_j|d_i)\)</span>训练出文档-主题和<span class="math inline">\(P(z_k|d_i)\)</span>主题-词项，如<span class="math inline">\(P(w_j|z_k)\)</span>下公式所示：</p>
<p><span class="math display">\[P(w_j|d_i)=\sum_{k=1}^KP(w_j|z_k)P(z_k|d_i)\]</span></p>
<p>故得到文档中每个词的生成概率为：</p>
<p><span class="math display">\[P(d_i,w_j)=P(d_i)P(w_j|d_i)=P(d_i)\sum_{k=1}^KP(w_j|z_k)P(z_k|d_i)\]</span></p>
<p>由于<span class="math inline">\(P(d_i)\)</span>可事先计算求出，而<span class="math inline">\(P(w_j|z_k)\)</span>和<span class="math inline">\(P(z_k|d_i)\)</span>未知，所以<span class="math inline">\(\theta=(P(w_j|z_k),P(z_k|d_i))\)</span>就是我们要估计的参数。</p>
<p>用什么方法进行估计呢，常用的参数估计方法有极大似然估计MLE、最大后验证估计MAP、贝叶斯估计等等。因为该待估计的参数中含有隐变量z，所以我们可以考虑EM算法。</p>
<p>。。。来日填补EM</p>
<h2 id="lda模型">LDA模型</h2>
<p>事实上，理解了pLSA模型，也就差不多快理解了LDA模型，因为LDA就是在pLSA的基础上加层贝叶斯框架，即LDA就是pLSA的贝叶斯版本（正因为LDA被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。</p>
<p>​ 在pLSA模型中，我们按照如下的步骤得到“文档-词项”的生成模型：</p>
<ol type="1">
<li>按照概率<span class="math inline">\(P(d_i)\)</span>选择一篇文档<span class="math inline">\(d_i\)</span></li>
<li>选定文档<span class="math inline">\(d_i\)</span>后，从主题分布中按照概率<span class="math inline">\(P(z_k|d_i)\)</span>选择一个隐含的主题类别<span class="math inline">\(z_k\)</span></li>
<li>选定<span class="math inline">\(z_k\)</span>后，从词分布中按照概率<span class="math inline">\(P(w_j|z_k)\)</span>选一个词<span class="math inline">\(w_j\)</span></li>
</ol>
<p>​ 下面，咱们对比下本文开头所述的LDA模型中一篇文档生成的方式是怎样的：</p>
<ol type="1">
<li>按照概率<span class="math inline">\(P(d_i)\)</span>选择一篇文档<span class="math inline">\(d_i\)</span></li>
<li>从狄利克雷分布<span class="math inline">\(\alpha\)</span>中取样生成文档<span class="math inline">\(d_i\)</span>的主题分布<span class="math inline">\(\theta_i\)</span></li>
<li>从主题分布<span class="math inline">\(\theta_i\)</span>中取样生成文档<span class="math inline">\(d_i\)</span>的第j个主题<span class="math inline">\(z_{i,j}\)</span></li>
<li>从狄利克雷分布<span class="math display">\[\beta\]</span>中取样生成主题对应的词语分布<span class="math inline">\(\phi_{z_{ij}}\)</span></li>
<li>从词语多项式分布中词语分布<span class="math inline">\(\phi_{z_{ij}}\)</span>采样生成最终词语<span class="math inline">\(w_{ij}\)</span></li>
</ol>
<p>从上面两个过程可以看出，LDA在PLSA的基础上，为主题分布和词分布分别加了两个Dirichlet先验。</p>
<p>继续拿之前讲解PLSA的例子进行具体说明。如前所述，在PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2018-08-05-14-50-02.png" /></p>
<p>而在LDA中，选主题和选词依然都是两个随机的过程，依然可能是先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后再从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。</p>
<p>​ 那PLSA跟LDA的区别在于什么地方呢？区别就在于：</p>
<ul>
<li>PLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}。</li>
<li>但在LDA中，主题分布和词分布不再唯一确定不变，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，即主题分布跟词分布由Dirichlet先验随机确定。</li>
</ul>
<p>...来日再来</p>
<h1 id="参考文献">参考文献</h1>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_july_v/article/details/41209515">通俗理解LDA主题模型</a></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/07/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/" rel="prev" title="深度学习算法-网络优化">
      <i class="fa fa-chevron-left"></i> 深度学习算法-网络优化
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/08/05/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E6%AD%BB%E9%94%81/" rel="next" title="操作系统-死锁">
      操作系统-死锁 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E8%A7%88"><span class="nav-number">1.</span> <span class="nav-text">基本概览</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%87%A0%E4%B8%AA%E5%88%86%E5%B8%83"><span class="nav-number">2.</span> <span class="nav-text">几个分布</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83"><span class="nav-number">2.1.</span> <span class="nav-text">二项分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83"><span class="nav-number">2.2.</span> <span class="nav-text">多项分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beta%E5%88%86%E5%B8%83"><span class="nav-number">2.3.</span> <span class="nav-text">Beta分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dirichlet%E5%88%86%E5%B8%83"><span class="nav-number">2.4.</span> <span class="nav-text">Dirichlet分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8Blda"><span class="nav-number">3.</span> <span class="nav-text">主题模型LDA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#unigram-model"><span class="nav-number">3.1.</span> <span class="nav-text">Unigram model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#plsa%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">PLSA模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%BB%E9%A2%98%E8%AF%8D%E7%94%9F%E6%88%90%E6%96%87%E6%A1%A3"><span class="nav-number">3.2.1.</span> <span class="nav-text">根据主题词生成文档</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E6%96%87%E6%A1%A3%E5%8F%8D%E6%8E%A8%E4%B8%BB%E9%A2%98%E5%88%86%E5%B8%83"><span class="nav-number">3.2.2.</span> <span class="nav-text">根据文档反推主题分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lda%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.3.</span> <span class="nav-text">LDA模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">jiayi797</p>
  <div class="site-description" itemprop="description">.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">150</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jiayi797</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
    <span class="post-count">| 博客共334.5k字</span>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
