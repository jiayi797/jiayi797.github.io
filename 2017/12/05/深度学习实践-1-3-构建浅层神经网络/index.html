<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
<!-- Referrer Policy调整致不蒜子单页面统计出错:https://senorui.top/posts/c33f.html -->
<meta name="referrer" content="no-referrer-when-downgrade">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiayi797.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="序言 本文主要参考自吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-3） 吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第三周“浅层神经网络”的课程作业。 本节的主要内容是：利用浅层神经网络实现平面数据分类">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习实践-1-3-构建浅层神经网络">
<meta property="og:url" content="http://jiayi797.github.io/about/2017/12/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-3-%E6%9E%84%E5%BB%BA%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="甲乙小朋友的房子">
<meta property="og:description" content="序言 本文主要参考自吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-3） 吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第三周“浅层神经网络”的课程作业。 本节的主要内容是：利用浅层神经网络实现平面数据分类">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-55-52.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-56-33.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-20-50-56.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-57-06.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-57-24.png">
<meta property="article:published_time" content="2017-12-05T13:58:36.000Z">
<meta property="article:modified_time" content="2018-12-17T07:42:02.000Z">
<meta property="article:author" content="jiayi797">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-55-52.png">

<link rel="canonical" href="http://jiayi797.github.io/about/2017/12/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-3-%E6%9E%84%E5%BB%BA%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习实践-1-3-构建浅层神经网络 | 甲乙小朋友的房子</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">甲乙小朋友的房子</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">甲乙小朋友很笨，但甲乙小朋友不会放弃</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jiayi797.github.io/about/2017/12/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-3-%E6%9E%84%E5%BB%BA%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jiayi797">
      <meta itemprop="description" content=".">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="甲乙小朋友的房子">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习实践-1-3-构建浅层神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-12-05 21:58:36" itemprop="dateCreated datePublished" datetime="2017-12-05T21:58:36+08:00">2017-12-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-12-17 15:42:02" itemprop="dateModified" datetime="2018-12-17T15:42:02+08:00">2018-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="序言">序言</h1>
<p>本文主要参考自<a target="_blank" rel="noopener" href="http://blog.csdn.net/koala_tree/article/details/78067464">吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-3）</a></p>
<p>吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第三周“浅层神经网络”的课程作业。</p>
<p>本节的主要内容是：利用浅层神经网络实现平面数据分类</p>
<span id="more"></span>
<h1 id="import的包">1 - import的包</h1>
<ul>
<li>numpy</li>
<li>sklearn</li>
<li>matplotlib</li>
<li>testCase - 提供了测试样例</li>
<li>pannar_utils - 一些有用的工具</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># 给np.random设定一个种子，这样随机数就固定了</span></span><br></pre></td></tr></table></figure>
<h1 id="数据集">2 - 数据集</h1>
<p>下面的代码生成了“花”的二类数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个函数原本在panar_utils.py里</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_planar_dataset</span>():</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    m = <span class="number">400</span> <span class="comment"># 样本数量</span></span><br><span class="line">    N = <span class="built_in">int</span>(m/<span class="number">2</span>) <span class="comment"># 每个类别的数量</span></span><br><span class="line">    D = <span class="number">2</span> <span class="comment"># 维度</span></span><br><span class="line">    <span class="comment"># 初始化X,Y</span></span><br><span class="line">    X = np.zeros((m,D))</span><br><span class="line">    Y = np.zeros((m,<span class="number">1</span>),dtype=<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    a = <span class="number">4</span> <span class="comment"># 花儿最大长度</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        ix = <span class="built_in">range</span>(N*j,N*(j+<span class="number">1</span>))</span><br><span class="line">        t = np.linspace(j*<span class="number">3.12</span>,(j+<span class="number">1</span>)*<span class="number">3.12</span>,N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">        r = a*np.sin(<span class="number">4</span>*t) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># radius</span></span><br><span class="line">        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">        Y[ix] = j</span><br><span class="line">        </span><br><span class="line">    X = X.T</span><br><span class="line">    Y = Y.T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line">X,Y = load_planar_dataset()</span><br><span class="line"><span class="built_in">print</span> X.shape,Y.shape</span><br></pre></td></tr></table></figure>
<pre><code>(2L, 400L) (1L, 400L)</code></pre>
<p>​</p>
<p>此时你得到了：</p>
<ul>
<li>一个numpy-array(matrix) X,包括特征（X1，X2）</li>
<li>一个numpy-arrya(vector) Y,包含一列标签（0或1）</li>
</ul>
<p>接下来用matplotlib将这个“花儿”数据集可视化，其中： - y = 0 -&gt; 红色 - y = 1 -&gt; 蓝色</p>
<p>我们的目标是建立一个模型，能将红色和蓝色分开</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-55-52.png" /></p>
<h1 id="简单的逻辑回归">简单的逻辑回归</h1>
<p>首先我们看看LR在这个问题上表现如何：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出LR的决策边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">&quot;Logistic Regression&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印准确率</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;LR的准确率是: %d &#x27;</span> % <span class="built_in">float</span>((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/<span class="built_in">float</span>(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">&#x27;% &#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>LR的准确率是: 47 % </code></pre>
<p>​</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-56-33.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以上的plot_decision_boundary()函数定义如下：</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span>(<span class="params">model, X, y</span>):</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding</span></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>, :].<span class="built_in">min</span>() - <span class="number">1</span>, X[<span class="number">0</span>, :].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>, :].<span class="built_in">min</span>() - <span class="number">1</span>, X[<span class="number">1</span>, :].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    <span class="comment"># Predict the function value for the whole grid</span></span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">    plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=y, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>
<h1 id="神经网络模型">神经网络模型</h1>
<p>我们本次<strong>模型</strong>为： <img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-20-50-56.png" /></p>
<p><strong>数学模型</strong> 对于一条样本<span class="math inline">\(x^{i}\)</span></p>
<p><span class="math display">\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}\]</span> <span class="math display">\[a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}\]</span> <span class="math display">\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}\]</span> <span class="math display">\[\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}\]</span> <span class="math display">\[y^{(i)}_{prediction} = \begin{cases} 1 &amp; \mbox{if } a^{[2](i)} &gt; 0.5 \\ 0 &amp; \mbox{otherwise } \end{cases}\tag{5}\]</span></p>
<p>通过上式计算出所有样本的预测误差，我们可以通过下式计算出误差函数： <span class="math display">\[J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}\]</span></p>
<p>回忆一下，计算神经网络的步骤为： 1. 定义网络结构 2. 初始化模型参数 3. 迭代 - 前向传播计算预测值 - 计算误差 - 后向传播计算梯度 - 根据梯度更新参数</p>
<h1 id="定义网络结构">定义网络结构</h1>
<p>约定： - n_x : 输入层的数据个数 - n_h : 隐藏层数 （此处设置为4） - n_y : 输出层的数据个数（类别个数）</p>
<p>可定义如下函数来获取以上三个数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span>(<span class="params">X, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]<span class="comment"># size of output layer</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br><span class="line">n_x, n_h, n_y = layer_sizes(X,Y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;n_x = &quot;</span>,n_x</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;n_h = &quot;</span>,n_h</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;n_y = &quot;</span>,n_y</span><br></pre></td></tr></table></figure>
<pre><code>n_x =  2
n_h =  4
n_y =  1</code></pre>
<h2 id="初始化参数">初始化参数</h2>
<p>需要初始化的参数主要是W和b</p>
<p>Exercise: Implement the function initialize_parameters().</p>
<p>Instructions: - Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. - You will initialize the weights matrices with random values. - Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). - You will initialize the bias vectors as zeros. - Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">2</span>) <span class="comment"># we set up a seed so that your output matches ours although the initialization is random.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>W1 = [[-0.41675785 -0.05626683]
 [-2.1361961   1.64027081]
 [-1.79343559 -0.84174737]
 [ 0.50288142 -1.24528809]]
b1 = [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
W2 = [[-1.05795222 -0.90900761  0.55145404  2.29220801]]
b2 = [[ 0.]]</code></pre>
<h2 id="迭代">迭代</h2>
<h3 id="前向传播">前向传播</h3>
<p><strong>问题</strong>: 实现 <code>forward_propagation()</code>.</p>
<p><strong>Instructions</strong>: - Look above at the mathematical representation of your classifier. - You can use the function <code>sigmoid()</code>. It is built-in (imported) in the notebook. - You can use the function <code>np.tanh()</code>. It is part of the numpy library. - The steps you have to implement are: 1. Retrieve each parameter from the dictionary "parameters" (which is the output of <code>initialize_parameters()</code>) by using <code>parameters[".."]</code>. 2. Implement Forward Propagation. Compute <span class="math inline">\(Z^{[1]}, A^{[1]}, Z^{[2]}\)</span> and <span class="math inline">\(A^{[2]}\)</span> (the vector of all your predictions on all the examples in the training set). - Values needed in the backpropagation are stored in "<code>cache</code>". The <code>cache</code> will be given as an input to the backpropagation function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    <span class="comment"># 参数获取</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    <span class="comment"># 计算预测值</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2 </span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,</span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,</span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,</span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br><span class="line"></span><br><span class="line">X_assess, parameters = forward_propagation_test_case()</span><br><span class="line">A2, cache = forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></span><br><span class="line"><span class="built_in">print</span>(np.mean(cache[<span class="string">&#x27;Z1&#x27;</span>]) ,np.mean(cache[<span class="string">&#x27;A1&#x27;</span>]),np.mean(cache[<span class="string">&#x27;Z2&#x27;</span>]),np.mean(cache[<span class="string">&#x27;A2&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>(-0.00049975577774199022, -0.00049696335323177901, 0.00043818745095914658, 0.50010954685243103)</code></pre>
<p>​</p>
<h3 id="计算误差">计算误差</h3>
<p>现在我们已经计算除了预测值A2，接下来我们需要计算本轮误差：</p>
<p><span class="math display">\[J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}\]</span></p>
<p><strong>Exercise</strong>: Implement <code>compute_cost()</code> to compute the value of the cost <span class="math inline">\(J\)</span>.</p>
<p><strong>Instructions</strong>: - There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented <span class="math inline">\(- \sum\limits_{i=0}^{m} y^{(i)}\log(a^{[2](i)})\)</span>: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2),Y)</span><br><span class="line">cost = - np.<span class="built_in">sum</span>(logprobs)                <span class="comment"># no need to use a for loop!</span></span><br></pre></td></tr></table></figure></p>
<p>(you can use either <code>np.multiply()</code> and then <code>np.sum()</code> or directly <code>np.dot()</code>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">A2, Y, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># 误差计算</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span>-A2), (<span class="number">1</span>-Y))</span><br><span class="line">    cost = -(<span class="number">1.0</span>/m)*np.<span class="built_in">sum</span>(logprobs)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(cost, <span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line">A2, Y_assess, parameters = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;cost = &quot;</span> + <span class="built_in">str</span>(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure>
<pre><code>cost = 0.692919893776</code></pre>
<p>​</p>
<h3 id="后向传播计算梯度">后向传播计算梯度</h3>
<p>Using the cache computed during forward propagation, you can now implement backward propagation.</p>
<p><strong>Question</strong>: Implement the function <code>backward_propagation()</code>.</p>
<p><strong>Instructions</strong>: Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You'll want to use the six equations on the right of this slide, since you are building a vectorized implementation.</p>
<p><span class="math inline">\(\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})\)</span></p>
<p>$ =  a^{[1] (i) T} $</p>
<p><span class="math inline">\(\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}\)</span></p>
<p>$ = W_2^T  * ( 1 - a^{[1] (i) 2}) $</p>
<p>$ =  X^T $</p>
<p><span class="math inline">\(\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}\)</span></p>
<ul>
<li>Note that <span class="math inline">\(*\)</span> denotes elementwise multiplication.</li>
<li>The notation you will use is common in deep learning coding:
<ul>
<li>dW1 = <span class="math inline">\(\frac{\partial \mathcal{J} }{ \partial W_1 }\)</span></li>
<li>db1 = <span class="math inline">\(\frac{\partial \mathcal{J} }{ \partial b_1 }\)</span></li>
<li>dW2 = <span class="math inline">\(\frac{\partial \mathcal{J} }{ \partial W_2 }\)</span></li>
<li>db2 = <span class="math inline">\(\frac{\partial \mathcal{J} }{ \partial b_2 }\)</span></li>
</ul></li>
<li>Tips:
<ul>
<li>To compute dZ1 you'll need to compute <span class="math inline">\(g^{[1]&#39;}(Z^{[1]})\)</span>. Since <span class="math inline">\(g^{[1]}(.)\)</span> is the tanh activation function, if <span class="math inline">\(a = g^{[1]}(z)\)</span> then <span class="math inline">\(g^{[1]&#39;}(z) = 1-a^2\)</span>. So you can compute <span class="math inline">\(g^{[1]&#39;}(Z^{[1]})\)</span> using <code>(1 - np.power(A1, 2))</code>.</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span></span><br><span class="line">    <span class="comment"># 获取参数</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    A1 = cache[<span class="string">&quot;A1&quot;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&quot;A2&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment"># 后向传播</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1.0</span>/m*np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.0</span>/m*np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2)*(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1.0</span>/m*np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.0</span>/m*np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,</span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,</span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW2 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db2 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;db2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>dW1 = [[ 0.01018708 -0.00708701]
 [ 0.00873447 -0.0060768 ]
 [-0.00530847  0.00369379]
 [-0.02206365  0.01535126]]
db1 = [[-0.00069728]
 [-0.00060606]
 [ 0.000364  ]
 [ 0.00151207]]
dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]
db2 = [[ 0.06589489]]</code></pre>
<h3 id="更新参数">更新参数</h3>
<p><strong>Question</strong>: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p>
<p><strong>General gradient descent rule</strong>: $ = - $ where <span class="math inline">\(\alpha\)</span> is the learning rate and <span class="math inline">\(\theta\)</span> represents a parameter.</p>
<p><strong>Illustration</strong>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span></span><br><span class="line">    <span class="comment"># 获取参数</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary &quot;grads&quot;</span></span><br><span class="line">    <span class="comment"># 获取梯度</span></span><br><span class="line">    dW1 = grads[<span class="string">&quot;dW1&quot;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&quot;db1&quot;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&quot;dW2&quot;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&quot;db2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[ -1.02420756e-06]
 [  1.27373948e-05]
 [  8.32996807e-07]
 [ -3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[ 0.00010457]]</code></pre>
<h3 id="将前面三步合在一起">将前面三步合在一起</h3>
<p><strong>Question</strong>: Build your neural network model in <code>nn_model()</code>.</p>
<p><strong>Instructions</strong>: The neural network model has to use the previous functions in the right order.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: nn_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.</span></span><br><span class="line">    <span class="comment"># 获取初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        <span class="comment"># 前向传播计算预测值</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        <span class="comment"># 计算误差</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        <span class="comment"># 后向传播计算梯度</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model_test_case</span>():</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X_assess = np.random.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    Y_assess = np.random.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X_assess, Y_assess</span><br><span class="line"></span><br><span class="line">X_assess, Y_assess = nn_model_test_case()</span><br><span class="line"><span class="built_in">print</span> X_assess</span><br><span class="line"><span class="built_in">print</span> Y_assess</span><br><span class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.62434536 -0.61175641 -0.52817175]
 [-1.07296862  0.86540763 -2.3015387 ]]
[[ 1.74481176 -0.7612069   0.3190391 ]]
Cost after iteration 0: -0.734104
Cost after iteration 1000: -inf
Cost after iteration 2000: -inf
Cost after iteration 3000: -inf
Cost after iteration 4000: -inf
Cost after iteration 5000: -inf
Cost after iteration 6000: -inf
Cost after iteration 7000: -inf
Cost after iteration 8000: -inf
Cost after iteration 9000: -inf
W1 = [[-7.53845806  1.20775367]
 [-4.25271792  5.29708473]
 [-7.53823957  1.20769882]
 [ 4.1479613  -5.35960029]]
b1 = [[ 3.81060333]
 [ 2.31388695]
 [ 3.81043858]
 [-2.32850837]]
W2 = [[-6012.41560745 -6036.77027646 -6010.58554698  6038.039225  ]]
b2 = [[-53.79862878]]</code></pre>
<h3 id="预测">预测</h3>
<p><strong>Question</strong>: Use your model to predict by building predict(). Use forward propagation to predict results.</p>
<p><strong>Reminder</strong>: predictions = $y_{prediction} =  () $</p>
<p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <code>X_new = (X &gt; threshold)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">parameters, X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parameters, X_assess = predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X_assess)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predictions mean = &quot;</span> + <span class="built_in">str</span>(np.mean(predictions)))</span><br></pre></td></tr></table></figure>
<pre><code>predictions mean = 0.666666666667</code></pre>
<p>​</p>
<h1 id="应用模型">应用模型</h1>
<p>将上面这个模型用在数据集上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">&quot;Decision Boundary for hidden layer size &quot;</span> + <span class="built_in">str</span>(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Accuracy: %d&#x27;</span> % <span class="built_in">float</span>((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/<span class="built_in">float</span>(Y.size)*<span class="number">100</span>) + <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Cost after iteration 0: 1.127380
Cost after iteration 1000: 0.288553
Cost after iteration 2000: 0.276386
Cost after iteration 3000: 0.268077
Cost after iteration 4000: 0.263069
Cost after iteration 5000: 0.259617
Cost after iteration 6000: 0.257070
Cost after iteration 7000: 0.255105
Cost after iteration 8000: 0.253534
Cost after iteration 9000: 0.252245
Accuracy: 91%</code></pre>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-57-06.png" /></p>
<h1 id="观测不同的隐藏层数对于模型的影响">观测不同的隐藏层数对于模型的影响</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This may take about 2 minutes to run</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> <span class="built_in">enumerate</span>(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Hidden Layer of size %d&#x27;</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = <span class="built_in">float</span>((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/<span class="built_in">float</span>(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;Accuracy for &#123;&#125; hidden units: &#123;&#125; %&quot;</span>.<span class="built_in">format</span>(n_h, accuracy))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy for 1 hidden units: 61.5 %
Accuracy for 2 hidden units: 70.5 %
Accuracy for 3 hidden units: 66.25 %
Accuracy for 4 hidden units: 90.75 %
Accuracy for 5 hidden units: 90.5 %
Accuracy for 20 hidden units: 92.0 %
Accuracy for 50 hidden units: 90.75 %</code></pre>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-05-21-57-24.png" /></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/12/04/%E7%AE%97%E6%B3%95-%E9%93%BE%E8%A1%A8/" rel="prev" title="算法-链表">
      <i class="fa fa-chevron-left"></i> 算法-链表
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" rel="next" title="深度学习算法-几种常见的卷积网络">
      深度学习算法-几种常见的卷积网络 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BA%8F%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">序言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#import%E7%9A%84%E5%8C%85"><span class="nav-number">2.</span> <span class="nav-text">1 - import的包</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.</span> <span class="nav-text">2 - 数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">4.</span> <span class="nav-text">简单的逻辑回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">神经网络模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">6.</span> <span class="nav-text">定义网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="nav-number">6.1.</span> <span class="nav-text">初始化参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3"><span class="nav-number">6.2.</span> <span class="nav-text">迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">6.2.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E8%AF%AF%E5%B7%AE"><span class="nav-number">6.2.2.</span> <span class="nav-text">计算误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="nav-number">6.2.3.</span> <span class="nav-text">后向传播计算梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0"><span class="nav-number">6.2.4.</span> <span class="nav-text">更新参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%86%E5%89%8D%E9%9D%A2%E4%B8%89%E6%AD%A5%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7"><span class="nav-number">6.2.5.</span> <span class="nav-text">将前面三步合在一起</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B"><span class="nav-number">6.2.6.</span> <span class="nav-text">预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.</span> <span class="nav-text">应用模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%82%E6%B5%8B%E4%B8%8D%E5%90%8C%E7%9A%84%E9%9A%90%E8%97%8F%E5%B1%82%E6%95%B0%E5%AF%B9%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">8.</span> <span class="nav-text">观测不同的隐藏层数对于模型的影响</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">jiayi797</p>
  <div class="site-description" itemprop="description">.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">150</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jiayi797</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
    <span class="post-count">| 博客共334.5k字</span>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
