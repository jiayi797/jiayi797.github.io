<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
<!-- Referrer Policy调整致不蒜子单页面统计出错:https://senorui.top/posts/c33f.html -->
<meta name="referrer" content="no-referrer-when-downgrade">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiayi797.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文主要参考自吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-4） 吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第四周“深层神经网络”的课程作业。 本节的主要内容是：实现L层的神经网络 大纲 首先完成一些helper function。然后再建立两层、多层神经网络：  两层和多层神经网络的">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习实践-1-4-1-构建深层神经网络">
<meta property="og:url" content="http://jiayi797.github.io/about/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-1-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="甲乙小朋友的房子">
<meta property="og:description" content="本文主要参考自吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-4） 吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第四周“深层神经网络”的课程作业。 本节的主要内容是：实现L层的神经网络 大纲 首先完成一些helper function。然后再建立两层、多层神经网络：  两层和多层神经网络的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-15-15-25.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-16-51-52.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-17-11-51.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-20-17-43.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-09-13-40-28.png">
<meta property="article:published_time" content="2017-12-10T07:06:06.000Z">
<meta property="article:modified_time" content="2018-12-17T07:42:02.000Z">
<meta property="article:author" content="jiayi797">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-15-15-25.png">

<link rel="canonical" href="http://jiayi797.github.io/about/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-1-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习实践-1-4-1-构建深层神经网络 | 甲乙小朋友的房子</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">甲乙小朋友的房子</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">甲乙小朋友很笨，但甲乙小朋友不会放弃</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://jiayi797.github.io/about/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-1-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="jiayi797">
      <meta itemprop="description" content=".">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="甲乙小朋友的房子">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习实践-1-4-1-构建深层神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-12-10 15:06:06" itemprop="dateCreated datePublished" datetime="2017-12-10T15:06:06+08:00">2017-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-12-17 15:42:02" itemprop="dateModified" datetime="2018-12-17T15:42:02+08:00">2018-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">深度学习算法</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文主要参考自<a target="_blank" rel="noopener" href="http://blog.csdn.net/koala_tree/article/details/78092337">吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-4）</a></p>
<p>吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第四周“深层神经网络”的课程作业。</p>
<p>本节的主要内容是：实现L层的神经网络</p>
<h1 id="大纲">大纲</h1>
<p>首先完成一些helper function。然后再建立两层、多层神经网络：</p>
<ul>
<li>两层和多层神经网络的初始化</li>
<li>实现前向传播模型
<ul>
<li>实现某一层的前向传播 - 输出<span class="math inline">\(Z^{[l]}\)</span></li>
<li>激活层已给出</li>
<li>将前两步结合</li>
<li>将前向传播迭代L-1次，然后将激活层加到最后，就完成了L层的前向传播</li>
</ul></li>
<li>计算误差</li>
<li>实现后向传播模型
<ul>
<li>实现某一层的后向传播</li>
<li>激活层的后向传播已给出</li>
<li>将前两步结合</li>
<li>将后向传播迭代L-1次，然后将激活层加到最后，完成了L层的后向传播</li>
</ul></li>
<li>更新参数</li>
</ul>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-15-15-25.png" /></p>
<span id="more"></span>
<h1 id="import">import</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="初始化">初始化</h1>
<ul>
<li>完成两层模型的初始化</li>
<li>完成多层模型的初始化</li>
</ul>
<h2 id="两层模型的初始化">两层模型的初始化</h2>
<ul>
<li>模型结构是： LINEAR - RELU - LINEAR - SIGMOID</li>
<li>用随机数初始化w</li>
<li>用0初始化b</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    n_x -- 输入层的大小</span></span><br><span class="line"><span class="string">    n_h -- 隐藏层的大小</span></span><br><span class="line"><span class="string">    n_y -- 输出层的大小</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dict ：</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters    </span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>W1 = [[ 0.01624345 -0.00611756 -0.00528172]
 [-0.01072969  0.00865408 -0.02301539]]
b1 = [[ 0.]
 [ 0.]]
W2 = [[ 0.01744812 -0.00761207]]
b2 = [[ 0.]]</code></pre>
<h2 id="l层模型的初始化">L层模型的初始化</h2>
<p>这个比较复杂。我们先来看一个X.shape = (12288,209)的例子:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><strong>W.shape</strong></th>
<th><strong>b.shape</strong></th>
<th style="text-align: center;"><strong>A</strong></th>
<th><strong>A.shape</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Layer 1</strong></td>
<td>(n[1],12288)</td>
<td>(n[1],1)</td>
<td style="text-align: center;">Z[1]=W[1]X+b[1]</td>
<td>(n[1],209)</td>
</tr>
<tr class="even">
<td><strong>Layer 2</strong></td>
<td>(n[2],n[1])</td>
<td>(n[2],1)</td>
<td style="text-align: center;">Z[2]=W[2]A[1]+b[2]</td>
<td>(n[2],209)</td>
</tr>
<tr class="odd">
<td>⋮</td>
<td>⋮</td>
<td>⋮</td>
<td style="text-align: center;">⋮</td>
<td>⋮</td>
</tr>
<tr class="even">
<td><strong>Layer L-1</strong></td>
<td>(n[L−1],n[L−2])</td>
<td>(n[L−1],1)</td>
<td style="text-align: center;">Z[L−1]=W[L−1]A[L−2]+b[L−1]</td>
<td>(n[L−1],209)</td>
</tr>
<tr class="odd">
<td><strong>Layer L</strong></td>
<td>(n[L],n[L−1])</td>
<td>(n[L],1)</td>
<td style="text-align: center;">Z[L]=W[L]A[L−1]+b[L]</td>
<td>(n[L],209)</td>
</tr>
</tbody>
</table>
<p>实现： - 模型结构是 [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID . 也就是说，有L-1层的ReLU激活函数，还有一层sigmoid输出 - w用随机初始化 - b用0初始化 - 将<span class="math inline">\(n^{[l]}\)</span>存入数组变量layer_dims中，代表第l层的n</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    layer_dims: 数组变量，代表每一层的维度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dict , 包含 &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span></span><br><span class="line"><span class="string">            Wl -- 权重矩阵，W1.shape =  (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">            bl -- 偏置向量，b1.shape =  (layer_dims[l], 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims) <span class="comment"># 网络的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment"># 初始化第l层参数</span></span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l-<span class="number">1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 验证</span></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], layer_dims[l-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>W1 = [[ 0.00319039 -0.0024937   0.01462108 -0.02060141 -0.00322417]
 [-0.00384054  0.01133769 -0.01099891 -0.00172428 -0.00877858]
 [ 0.00042214  0.00582815 -0.01100619  0.01144724  0.00901591]
 [ 0.00502494  0.00900856 -0.00683728 -0.0012289  -0.00935769]]
b1 = [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
W2 = [[-0.00267888  0.00530355 -0.00691661 -0.00396754]
 [-0.00687173 -0.00845206 -0.00671246 -0.00012665]
 [-0.0111731   0.00234416  0.01659802  0.00742044]]
b2 = [[ 0.]
 [ 0.]
 [ 0.]]</code></pre>
<h1 id="前向传播">前向传播</h1>
<p>按照如下顺序实现： - LINEAR - LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. - [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model)</p>
<h2 id="linear">LINEAR</h2>
<p>线性模型如下：</p>
<p><span class="math display">\[Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}\]</span></p>
<p>where <span class="math inline">\(A^{[0]} = X\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span>(<span class="params">A,W,b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现前向传播的线性部分</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    A -- 上一层的输出，也是本层的输入</span></span><br><span class="line"><span class="string">    W -- 权重矩阵 ， W.shape = (n_(l-1), n_l)</span></span><br><span class="line"><span class="string">    b -- 偏置向量 ， b.shape = (n_l , 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns : </span></span><br><span class="line"><span class="string">    Z -- 激活函数的输入，也叫&quot;pre-activation parameter&quot;</span></span><br><span class="line"><span class="string">    cache - dict，包含A,W,b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 线性部分</span></span><br><span class="line">    Z = np.dot(W,A) + b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 暂存</span></span><br><span class="line">    cache = (A, W, b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward_test_case</span>():</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.array([[-1.02387576, 1.12397796],</span></span><br><span class="line"><span class="string"> [-1.62328545, 0.64667545],</span></span><br><span class="line"><span class="string"> [-1.74314104, -0.59664964]])</span></span><br><span class="line"><span class="string">    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])</span></span><br><span class="line"><span class="string">    b = np.array([[1]])</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, W, b</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">A, W, b = linear_forward_test_case()</span><br><span class="line"></span><br><span class="line">Z, linear_cache = linear_forward(A, W, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Z = &quot;</span> + <span class="built_in">str</span>(Z))</span><br></pre></td></tr></table></figure>
<pre><code>Z = [[ 3.26295337 -1.23429987]]</code></pre>
<p>​</p>
<h2 id="linear---activation">LINEAR -&gt; ACTIVATION</h2>
<p>本层的公式是：</p>
<p><span class="math inline">\(A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\)</span></p>
<p>其中,g()有两种选择：</p>
<p><strong>sigmod</strong> ：</p>
<p><span class="math inline">\(\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)}}\)</span></p>
<p>sigmod的实现如下所示</p>
<p>输出: - A - cache = z</p>
<p><strong>ReLU</strong> :</p>
<p><span class="math inline">\(A = RELU(Z) = max(0, Z)\)</span></p>
<p>ReLU的实现如下所示 输出： - A - cache = Z</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">Z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the sigmoid activation in numpy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- numpy array of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class="line"><span class="string">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    A = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    cache = Z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">Z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the RELU function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- Output of the linear layer, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape)</span><br><span class="line"></span><br><span class="line">    cache = Z </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现LINEAR-&gt;ACTIVATION层</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span>(<span class="params">A_prev, W, b, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    LINEAR-&gt;ACTIVATION的实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    A_prev -- 上一层的输出</span></span><br><span class="line"><span class="string">    W -- 权重矩阵 ， W.shape = (n_l, n_(l-1))</span></span><br><span class="line"><span class="string">    b -- 偏置向量 ， b.shape = (n_l , 1)</span></span><br><span class="line"><span class="string">    activation -- 激活的名称，&quot;sigmoid&quot;或&quot;relu&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- 激活层输出，也叫post-activation value</span></span><br><span class="line"><span class="string">    cache  -- python dict, 包含 &quot;linear_cache&quot; 和 &quot;activation_cache&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 线性输出</span></span><br><span class="line">    Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 激活</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.array([[-1.02387576, 1.12397796],</span></span><br><span class="line"><span class="string"> [-1.62328545, 0.64667545],</span></span><br><span class="line"><span class="string"> [-1.74314104, -0.59664964]])</span></span><br><span class="line"><span class="string">    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])</span></span><br><span class="line"><span class="string">    b = 5</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    A_prev = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> A_prev, W, b</span><br><span class="line"></span><br><span class="line">A_prev, W, b = linear_activation_forward_test_case()</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;With sigmoid: A = &quot;</span> + <span class="built_in">str</span>(A))</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;With ReLU: A = &quot;</span> + <span class="built_in">str</span>(A))</span><br></pre></td></tr></table></figure>
<pre><code>With sigmoid: A = [[ 0.96890023  0.11013289]]
With ReLU: A = [[ 3.43896131  0.        ]]</code></pre>
<h2 id="l-model-forward">L-Model Forward</h2>
<ul>
<li>用RELU，重复之前的linear_activation_forward，L-1次</li>
<li>最后输入SIGMOID 的linear_activation_forward</li>
</ul>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-16-51-52.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现后向传播的[LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID步骤</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    X -- 数据集</span></span><br><span class="line"><span class="string">    parameters -- initialize_parameters_deep()的输出，是多层网络初始化后的参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    AL -- 最终输出</span></span><br><span class="line"><span class="string">    caches -- 每层的caches：</span></span><br><span class="line"><span class="string">            1 ~ L - 1层 的 linear_relu_forward()</span></span><br><span class="line"><span class="string">            L 层 的 linear_sigmoid_forward()</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    caches = []</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) / <span class="number">2</span></span><br><span class="line">    A = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1~ L-1 层，迭代</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)], parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)], <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># L 层     </span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(L)], parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(L)], <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.array([[-1.02387576, 1.12397796],</span></span><br><span class="line"><span class="string"> [-1.62328545, 0.64667545],</span></span><br><span class="line"><span class="string"> [-1.74314104, -0.59664964]])</span></span><br><span class="line"><span class="string">    parameters = &#123;&#x27;W1&#x27;: np.array([[ 1.62434536, -0.61175641, -0.52817175],</span></span><br><span class="line"><span class="string">        [-1.07296862,  0.86540763, -2.3015387 ]]),</span></span><br><span class="line"><span class="string"> &#x27;W2&#x27;: np.array([[ 1.74481176, -0.7612069 ]]),</span></span><br><span class="line"><span class="string"> &#x27;b1&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]]),</span></span><br><span class="line"><span class="string"> &#x27;b2&#x27;: np.array([[ 0.]])&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X = np.random.randn(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取X和初始化参数</span></span><br><span class="line">X, parameters = L_model_forward_test_case()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;X.shape = &quot;</span> + <span class="built_in">str</span>(X.shape) , <span class="string">&quot;，即&quot;</span>,X.shape[<span class="number">0</span>],<span class="string">&quot;行&quot;</span>,X.shape[<span class="number">1</span>],<span class="string">&quot;列&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">AL, caches = L_model_forward(X, parameters)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;AL = &quot;</span> + <span class="built_in">str</span>(AL))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Length of caches list = &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(caches)))</span><br></pre></td></tr></table></figure>
<pre><code>X.shape = (4L, 2L) ，即 4 行 2 列
AL = [[ 0.17007265  0.2524272 ]]
Length of caches list = 2</code></pre>
<h1 id="计算误差">计算误差</h1>
<p>误差公式如下：</p>
<p><span class="math display">\[-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">AL, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现公式7的误差计算</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    AL -- 上一步计算出的结果</span></span><br><span class="line"><span class="string">    Y -- 实际的label</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns : </span></span><br><span class="line"><span class="string">    cost -- 交叉熵误差</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算</span></span><br><span class="line">    cost = -np.<span class="built_in">sum</span>(np.multiply(np.log(AL),Y) + np.multiply(np.log(<span class="number">1</span> - AL), <span class="number">1</span> - Y)) / m</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_test_case</span>():</span></span><br><span class="line">    Y = np.asarray([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">    aL = np.array([[<span class="number">.8</span>,<span class="number">.9</span>,<span class="number">0.4</span>]])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y, aL</span><br><span class="line"></span><br><span class="line">Y, AL = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;cost = &quot;</span> + <span class="built_in">str</span>(compute_cost(AL, Y)))</span><br></pre></td></tr></table></figure>
<pre><code>cost = 0.414931599615</code></pre>
<p>​</p>
<h1 id="后向传播">后向传播</h1>
<p>后向传播是用来计算梯度的</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-17-11-51.png" /></p>
<p>紫色方块代表前向传播 红色方块代表后向传播</p>
<p>我们的目标是计算出<span class="math inline">\(dw^{[l]}和db^{[l]}, l = 1,2,...L\)</span>，以便之后更新w和b。</p>
<p>为了计算<span class="math inline">\(dw^{[1]}和db^{[1]}\)</span>，要使用如下链式法则：</p>
<p><span class="math inline">\(dw^{[1]}=\frac{dL}{dw^{[1]}}=\frac{dL}{dz^{[1]}} \times \frac{dz}{dw^{[1]}}=dz^{[1]}\times \frac{dz}{dw^{[1]}}\)</span></p>
<p><span class="math inline">\(db^{[1]}=\frac{dL}{db^{[1]}} = \frac{dL}{dz^{[1]}} \times \frac{dz^{[1]}}{db^{[1]}} = dz^{[1]} \times \frac{dz^{[1]}}{db^{[1]}}\)</span></p>
<p>因此我们首先要算出<span class="math inline">\(dz^{[1]}\)</span> :</p>
<p><span class="math display">\[\frac{dL(a^{[2]},y)}{dz^{[1]}}=\frac{dL(a^{[2]},y)}{da^{[2]}}\frac{da^{[2]}}{dz^{[2]}}\frac{dz^{[2]}}{da^{[1]}}\frac{da^{[1]}}{dz^{[1]}}\]</span></p>
<p>而要算出<span class="math inline">\(dz^{[1]}\)</span>，由上公示可以看出，我们必须先计算<span class="math inline">\(dz^{[2]}\)</span>等</p>
<p>因此此过程叫做<strong>后向传播</strong></p>
<p>总之，后向传播需要完成： - LINEAR - LINEAR -&gt; ACTIVATION - [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</p>
<h2 id="linear-1">LINEAR</h2>
<p>对于第l层来说，这一层的Linear部分是：<span class="math inline">\(Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\)</span></p>
<p>假设此时你已经计算好了 <span class="math inline">\(dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}\)</span></p>
<p>你接下来想要得到<span class="math inline">\((dW^{[l]}, db^{[l]} dA^{[l-1]})\)</span></p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-20-17-43.png" /></p>
<p>我们可以通过如下的公式，通过<span class="math inline">\(dZ^{[l]}\)</span>来计算出这三个东西<span class="math inline">\((dW^{[l]}, db^{[l]}, dA^{[l-1]})\)</span>:</p>
<p><span class="math display">\[ dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{8}\]</span> <span class="math display">\[ db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\tag{9}\]</span> <span class="math display">\[ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{10}\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span>(<span class="params">dZ, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现后向传播的linear部分</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    dZ -- dL/dz</span></span><br><span class="line"><span class="string">    cache -- 三元组(A_prev, W, b) , 来自于当前层的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 上一层A的梯度</span></span><br><span class="line"><span class="string">    dW -- w的梯度</span></span><br><span class="line"><span class="string">    db -- b的梯度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算dw，db，dA_prev</span></span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.<span class="built_in">sum</span>(dZ, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) / m </span><br><span class="line">    dA_prev = np.dot(W.T, dZ) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],</span></span><br><span class="line"><span class="string">       [-1.62328545,  0.64667545],</span></span><br><span class="line"><span class="string">       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    dZ = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    linear_cache = (A, W, b)</span><br><span class="line">    <span class="keyword">return</span> dZ, linear_cache </span><br><span class="line"></span><br><span class="line">dZ, linear_cache = linear_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA_prev = &quot;</span>+ <span class="built_in">str</span>(dA_prev))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW = &quot;</span> + <span class="built_in">str</span>(dW))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db = &quot;</span> + <span class="built_in">str</span>(db))</span><br></pre></td></tr></table></figure>
<pre><code>dA_prev = [[ 0.51822968 -0.19517421]
 [-0.40506361  0.15255393]
 [ 2.37496825 -0.89445391]]
dW = [[-0.10076895  1.40685096  1.64992505]]
db = [[ 0.50629448]]</code></pre>
<h2 id="linear---activation-1">LINEAR -&gt; ACTIVATION</h2>
<p>本节要加入后向传播中的activation部分：</p>
<p>假设<span class="math inline">\(g(.)\)</span>是激活函数，</p>
<p>而下面给出的两个函数：<code>sigmoid_backward</code> 和 <code>relu_backward</code> 计算了<span class="math inline">\(dL/dz\)</span> : <span class="math display">\[dZ^{[l]} = dA^{[l]} * g&#39;(Z^{[l]}) \tag{11}\]</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span>(<span class="params">dA, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single RELU unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- &#x27;Z&#x27; where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    dZ = np.array(dA, copy=<span class="literal">True</span>) <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># When z &lt;= 0, you should set dz to 0 as well. </span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span>(<span class="params">dA, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- &#x27;Z&#x27; where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span>(<span class="params">dA, cache, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    后向传播的LINEAR-&gt;ACTIVATION实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    dA -- 当前层的输出A的导数</span></span><br><span class="line"><span class="string">    cache -- 二元组(linear_cache, activation_cache)，也就是之前前向传播计算时的cache</span></span><br><span class="line"><span class="string">    activation -- 本层的激活函数，是&quot;sigmoid&quot;或&quot;relu&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns :</span></span><br><span class="line"><span class="string">    dA_prev -- 上一层输出的导数</span></span><br><span class="line"><span class="string">    dW -- 本层W的导数</span></span><br><span class="line"><span class="string">    db -- 本层b的导数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    <span class="comment"># 计算dZ</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA,activation_cache)</span><br><span class="line">    <span class="comment"># 已知dZ, 计算dA_prev, dW, db</span></span><br><span class="line">    dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    dA = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    Z = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache = (A, W, b)</span><br><span class="line">    activation_cache = Z</span><br><span class="line">    linear_activation_cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA, linear_activation_cache</span><br><span class="line"></span><br><span class="line">AL, linear_activation_cache = linear_activation_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;sigmoid:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA_prev = &quot;</span>+ <span class="built_in">str</span>(dA_prev))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW = &quot;</span> + <span class="built_in">str</span>(dW))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db = &quot;</span> + <span class="built_in">str</span>(db) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;relu:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA_prev = &quot;</span>+ <span class="built_in">str</span>(dA_prev))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW = &quot;</span> + <span class="built_in">str</span>(dW))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db = &quot;</span> + <span class="built_in">str</span>(db))</span><br></pre></td></tr></table></figure>
<pre><code>sigmoid:
dA_prev = [[ 0.11017994  0.01105339]
 [ 0.09466817  0.00949723]
 [-0.05743092 -0.00576154]]
dW = [[ 0.10266786  0.09778551 -0.01968084]]
db = [[-0.05729622]]

(1L, 2L)
(1L, 2L)
relu:
dA_prev = [[ 0.44090989 -0.        ]
 [ 0.37883606 -0.        ]
 [-0.2298228   0.        ]]
dW = [[ 0.44513824  0.37371418 -0.10478989]]
db = [[-0.20837892]]</code></pre>
<h2 id="l-model-backward">L-Model Backward</h2>
<p>接下来就该将后向传播应用在整个网络了。步骤如下：</p>
<ol type="1">
<li>前向传播 - <code>L_model_forward()</code>,在每一层都存下了(X,W,b,z)</li>
<li>后向传播 - <code>L_model_backward()</code>，利用之前存的值，一层层向前计算导数</li>
</ol>
<p>后向传播计算导数的流程如下图所示： <img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-09-13-40-28.png" /></p>
<p><strong>初始化部分</strong></p>
<p>对于贯穿网络的后向传播来说，我们知道输出是<span class="math inline">\(A^{[L]} = \sigma(Z^{[L]})\)</span>. 我们需要计算出 ： <code>dAL</code> <span class="math inline">\(= \frac{\partial \mathcal{L}}{\partial A^{[L]}}\)</span>.</p>
<p>为了完成这个目标，我们用如下公式实现： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># 相对于AL的成本衍生物</span></span><br></pre></td></tr></table></figure> 这个公式计算了LINEAR-&gt;SIGMOID后向传播部分</p>
<p>接下来就该计算LINEAR-&gt;RELU后向传播部分了，这一部分需要存储每一步的dA, dW, db，用如下公式实现： <span class="math display">\[grads[&quot;dW&quot; + str(l)] = dW^{[l]}\tag{15} \]</span> 例如，l=3，就将dw3 存储在<code>grads["dw3"]</code>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span>(<span class="params">AL, Y, chache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID 的后向传播的实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    AL -- 概率向量，是前向传播L_model_forward()的输出</span></span><br><span class="line"><span class="string">    Y  -- 标签向量</span></span><br><span class="line"><span class="string">    caches -- caches列表：</span></span><br><span class="line"><span class="string">                     1 ~ L - 1 : relu的linear_activation_forward()的caches</span></span><br><span class="line"><span class="string">                     L         : sigmoid 的 linear_activation_forward()的caches</span></span><br><span class="line"><span class="string">                     </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    grads -- 梯度dict :</span></span><br><span class="line"><span class="string">        grads[&quot;dA&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">        grads[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">        grads[&quot;db&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches)</span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#后向传播的初始化</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算dAL,dWL,dbL</span></span><br><span class="line">    <span class="comment"># 第L层 - (SIGMOID -&gt; LINEAR)  - 的梯度</span></span><br><span class="line">    <span class="comment"># 输入 ：AL, Y, caches</span></span><br><span class="line">    <span class="comment"># 输出 ：grads[&quot;dAL&quot;], grads[&quot;dWL&quot;]</span></span><br><span class="line">    current_cache = caches[L - <span class="number">1</span>]</span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = linear_activation_backward(dAL,current_cache, <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算dAl,dWl,dbl , l = 1~L-1</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L-<span class="number">1</span>)):</span><br><span class="line">        <span class="comment"># 第l层：(RELU -&gt; LINEAR) 的梯度</span></span><br><span class="line">        <span class="comment"># 输入 ： &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. </span></span><br><span class="line">        <span class="comment"># 输出: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">2</span>)], current_cache, <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = db_temp</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.random.rand(3,2)</span></span><br><span class="line"><span class="string">    Y = np.array([[1, 1]])</span></span><br><span class="line"><span class="string">    parameters = &#123;&#x27;W1&#x27;: np.array([[ 1.78862847,  0.43650985,  0.09649747]]), &#x27;b1&#x27;: np.array([[ 0.]])&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],</span></span><br><span class="line"><span class="string">           [ 0.02738759,  0.67046751],</span></span><br><span class="line"><span class="string">           [ 0.4173048 ,  0.55868983]]),</span></span><br><span class="line"><span class="string">    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),</span></span><br><span class="line"><span class="string">    np.array([[ 0.]])),</span></span><br><span class="line"><span class="string">   np.array([[ 0.41791293,  1.91720367]]))])</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    AL = np.random.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    Y = np.array([[<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    A1 = np.random.randn(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    Z1 = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache_activation_1 = ((A1, W1, b1), Z1)</span><br><span class="line"></span><br><span class="line">    A2 = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    Z2 = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache_activation_2 = ( (A2, W2, b2), Z2)</span><br><span class="line"></span><br><span class="line">    caches = (linear_cache_activation_1, linear_cache_activation_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, Y, caches</span><br><span class="line"></span><br><span class="line">AL, Y_assess, caches = L_model_backward_test_case()</span><br><span class="line">grads = L_model_backward(AL, Y_assess, caches)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;dA1&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>(3L, 2L)
(3L, 2L)
dW1 = [[ 0.41010002  0.07807203  0.13798444  0.10502167]
 [ 0.          0.          0.          0.        ]
 [ 0.05283652  0.01005865  0.01777766  0.0135308 ]]
db1 = [[-0.22007063]
 [ 0.        ]
 [-0.02835349]]
dA1 = [[ 0.          0.52257901]
 [ 0.         -0.3269206 ]
 [ 0.         -0.32070404]
 [ 0.         -0.74079187]]</code></pre>
<h1 id="更新参数">更新参数</h1>
<p>在这一部分我们用以上模型更新参数：</p>
<p><span class="math display">\[ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{16}\]</span> <span class="math display">\[ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{17}\]</span></p>
<p>其中，<span class="math inline">\(\alpha\)</span>是学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span>(<span class="params">parameters, grads, learning_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用梯度下降更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    parameters -- python dict，里面包含一些参数</span></span><br><span class="line"><span class="string">    grads -- python dict, 包含梯度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dict, 包含更新后的参数：</span></span><br><span class="line"><span class="string">                  parameters[&quot;W&quot; + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters[&quot;b&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    L = <span class="built_in">len</span>(parameters) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] = parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] = parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    parameters = &#123;&#x27;W1&#x27;: np.array([[ 1.78862847,  0.43650985,  0.09649747],</span></span><br><span class="line"><span class="string">        [-1.8634927 , -0.2773882 , -0.35475898],</span></span><br><span class="line"><span class="string">        [-0.08274148, -0.62700068, -0.04381817],</span></span><br><span class="line"><span class="string">        [-0.47721803, -1.31386475,  0.88462238]]),</span></span><br><span class="line"><span class="string"> &#x27;W2&#x27;: np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],</span></span><br><span class="line"><span class="string">        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],</span></span><br><span class="line"><span class="string">        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),</span></span><br><span class="line"><span class="string"> &#x27;W3&#x27;: np.array([[-1.02378514, -0.7129932 ,  0.62524497],</span></span><br><span class="line"><span class="string">        [-0.16051336, -0.76883635, -0.23003072]]),</span></span><br><span class="line"><span class="string"> &#x27;b1&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.],</span></span><br><span class="line"><span class="string">        [ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]]),</span></span><br><span class="line"><span class="string"> &#x27;b2&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]]),</span></span><br><span class="line"><span class="string"> &#x27;b3&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]])&#125;</span></span><br><span class="line"><span class="string">    grads = &#123;&#x27;dW1&#x27;: np.array([[ 0.63070583,  0.66482653,  0.18308507],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ]]),</span></span><br><span class="line"><span class="string"> &#x27;dW2&#x27;: np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ,  0.        ]]),</span></span><br><span class="line"><span class="string"> &#x27;dW3&#x27;: np.array([[-1.40260776,  0.        ,  0.        ]]),</span></span><br><span class="line"><span class="string"> &#x27;da1&#x27;: np.array([[ 0.70760786,  0.65063504],</span></span><br><span class="line"><span class="string">        [ 0.17268975,  0.15878569],</span></span><br><span class="line"><span class="string">        [ 0.03817582,  0.03510211]]),</span></span><br><span class="line"><span class="string"> &#x27;da2&#x27;: np.array([[ 0.39561478,  0.36376198],</span></span><br><span class="line"><span class="string">        [ 0.7674101 ,  0.70562233],</span></span><br><span class="line"><span class="string">        [ 0.0224596 ,  0.02065127],</span></span><br><span class="line"><span class="string">        [-0.18165561, -0.16702967]]),</span></span><br><span class="line"><span class="string"> &#x27;da3&#x27;: np.array([[ 0.44888991,  0.41274769],</span></span><br><span class="line"><span class="string">        [ 0.31261975,  0.28744927],</span></span><br><span class="line"><span class="string">        [-0.27414557, -0.25207283]]),</span></span><br><span class="line"><span class="string"> &#x27;db1&#x27;: 0.75937676204411464,</span></span><br><span class="line"><span class="string"> &#x27;db2&#x27;: 0.86163759922811056,</span></span><br><span class="line"><span class="string"> &#x27;db3&#x27;: -0.84161956022334572&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    dW1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    db1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    dW2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    db2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,</span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,</span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters, grads</span><br><span class="line"></span><br><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;W1 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;b1 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;W2 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;b2 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W2 = [[-0.55569196  0.0354055   1.32964895]]
b2 = [[-0.84610769]]</code></pre>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" rel="prev" title="深度学习算法-几种常见的卷积网络">
      <i class="fa fa-chevron-left"></i> 深度学习算法-几种常见的卷积网络
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-2-%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E7%8C%AB/" rel="next" title="深度学习实践-1-4-2-用神经网络识别猫">
      深度学习实践-1-4-2-用神经网络识别猫 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E7%BA%B2"><span class="nav-number">1.</span> <span class="nav-text">大纲</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#import"><span class="nav-number">2.</span> <span class="nav-text">import</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%A4%E5%B1%82%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.1.</span> <span class="nav-text">两层模型的初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#l%E5%B1%82%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.2.</span> <span class="nav-text">L层模型的初始化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.</span> <span class="nav-text">前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#linear"><span class="nav-number">4.1.</span> <span class="nav-text">LINEAR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linear---activation"><span class="nav-number">4.2.</span> <span class="nav-text">LINEAR -&gt; ACTIVATION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#l-model-forward"><span class="nav-number">4.3.</span> <span class="nav-text">L-Model Forward</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E8%AF%AF%E5%B7%AE"><span class="nav-number">5.</span> <span class="nav-text">计算误差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">6.</span> <span class="nav-text">后向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#linear-1"><span class="nav-number">6.1.</span> <span class="nav-text">LINEAR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linear---activation-1"><span class="nav-number">6.2.</span> <span class="nav-text">LINEAR -&gt; ACTIVATION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#l-model-backward"><span class="nav-number">6.3.</span> <span class="nav-text">L-Model Backward</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0"><span class="nav-number">7.</span> <span class="nav-text">更新参数</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">jiayi797</p>
  <div class="site-description" itemprop="description">.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">150</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jiayi797</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
    <span class="post-count">| 博客共334.5k字</span>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
