<!doctype html>




<html class="theme-next pisces" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="true" />







  <meta name="baidu-site-verification" content="q1zwhBKKPA" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="本文主要参考自吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-4） 吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第四周“深层神经网络”的课程作业。 本节的主要内容是：实现L层的神经网络 大纲首先完成一些helper function。然后再建立两层、多层神经网络：  两层和多层神经网络的初始化 实现前">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习实践-1-4-1-构建深层神经网络">
<meta property="og:url" content="http://jiayi797.github.io/about/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-1-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="甲乙小朋友的房子">
<meta property="og:description" content="本文主要参考自吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-4） 吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第四周“深层神经网络”的课程作业。 本节的主要内容是：实现L层的神经网络 大纲首先完成一些helper function。然后再建立两层、多层神经网络：  两层和多层神经网络的初始化 实现前">
<meta property="og:locale">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-15-15-25.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-16-51-52.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-17-11-51.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-20-17-43.png">
<meta property="og:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-09-13-40-28.png">
<meta property="article:published_time" content="2017-12-10T07:06:06.000Z">
<meta property="article:modified_time" content="2018-12-17T07:42:02.000Z">
<meta property="article:author" content="jiayi797">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-15-15-25.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: false,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://jiayi797.github.io/about/2017/12/10/深度学习实践-1-4-1-构建深层神经网络/"/>





  <title>深度学习实践-1-4-1-构建深层神经网络 | 甲乙小朋友的房子</title>
<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-110169171-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9856596edaab494b299151eb0e9bb214";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>











  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">甲乙小朋友的房子</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">甲乙小朋友很笨，但甲乙小朋友不会放弃</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            时光机
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            琐碎
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://jiayi797.github.io/about/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-1-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="甲乙小朋友的房子">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习实践-1-4-1-构建深层神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-10T07:06:06+00:00">
                2017-12-10
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-17T07:42:02+00:00">
                2018-12-17
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习算法</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index">
                    <span itemprop="name">神经网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-comment-o"></i>
              </span>
              
                <a href="/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-1-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#SOHUCS" itemprop="discussionUrl">
                  <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2017/12/10/深度学习实践-1-4-1-构建深层神经网络/" itemprop="commentsCount"></span>
                </a>
              
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计</span>
                
                <span title="字数统计">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要参考自<a target="_blank" rel="noopener" href="http://blog.csdn.net/koala_tree/article/details/78092337">吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-4）</a></p>
<p>吴恩达Coursera课程 DeepLearning.ai 编程作业系列，本文为《神经网络与深度学习》部分的第四周“深层神经网络”的课程作业。</p>
<p>本节的主要内容是：实现L层的神经网络</p>
<h1 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h1><p>首先完成一些helper function。然后再建立两层、多层神经网络：</p>
<ul>
<li>两层和多层神经网络的初始化</li>
<li>实现前向传播模型<ul>
<li>实现某一层的前向传播 - 输出$Z^{[l]}$</li>
<li>激活层已给出</li>
<li>将前两步结合</li>
<li>将前向传播迭代L-1次，然后将激活层加到最后，就完成了L层的前向传播</li>
</ul>
</li>
<li>计算误差</li>
<li>实现后向传播模型<ul>
<li>实现某一层的后向传播</li>
<li>激活层的后向传播已给出</li>
<li>将前两步结合</li>
<li>将后向传播迭代L-1次，然后将激活层加到最后，完成了L层的后向传播</li>
</ul>
</li>
<li>更新参数</li>
</ul>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-15-15-25.png"> </p>
<span id="more"></span>

<h1 id="import"><a href="#import" class="headerlink" title="import"></a>import</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>



<h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><ul>
<li>完成两层模型的初始化</li>
<li>完成多层模型的初始化</li>
</ul>
<h2 id="两层模型的初始化"><a href="#两层模型的初始化" class="headerlink" title="两层模型的初始化"></a>两层模型的初始化</h2><ul>
<li>模型结构是： LINEAR - RELU - LINEAR - SIGMOID</li>
<li>用随机数初始化w</li>
<li>用0初始化b</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    n_x -- 输入层的大小</span></span><br><span class="line"><span class="string">    n_h -- 隐藏层的大小</span></span><br><span class="line"><span class="string">    n_y -- 输出层的大小</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dict ：</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters    </span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 0.01624345 -0.00611756 -0.00528172]
 [-0.01072969  0.00865408 -0.02301539]]
b1 = [[ 0.]
 [ 0.]]
W2 = [[ 0.01744812 -0.00761207]]
b2 = [[ 0.]]
</code></pre>
<h2 id="L层模型的初始化"><a href="#L层模型的初始化" class="headerlink" title="L层模型的初始化"></a>L层模型的初始化</h2><p>这个比较复杂。我们先来看一个X.shape = (12288,209)的例子:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>W.shape</strong></th>
<th><strong>b.shape</strong></th>
<th align="center"><strong>A</strong></th>
<th><strong>A.shape</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Layer 1</strong></td>
<td>(n[1],12288)</td>
<td>(n[1],1)</td>
<td align="center">Z[1]=W[1]X+b[1]</td>
<td>(n[1],209)</td>
</tr>
<tr>
<td><strong>Layer 2</strong></td>
<td>(n[2],n[1])</td>
<td>(n[2],1)</td>
<td align="center">Z[2]=W[2]A[1]+b[2]</td>
<td>(n[2],209)</td>
</tr>
<tr>
<td>⋮</td>
<td>⋮</td>
<td>⋮</td>
<td align="center">⋮</td>
<td>⋮</td>
</tr>
<tr>
<td><strong>Layer L-1</strong></td>
<td>(n[L−1],n[L−2])</td>
<td>(n[L−1],1)</td>
<td align="center">Z[L−1]=W[L−1]A[L−2]+b[L−1]</td>
<td>(n[L−1],209)</td>
</tr>
<tr>
<td><strong>Layer L</strong></td>
<td>(n[L],n[L−1])</td>
<td>(n[L],1)</td>
<td align="center">Z[L]=W[L]A[L−1]+b[L]</td>
<td>(n[L],209)</td>
</tr>
</tbody></table>
<p>实现：</p>
<ul>
<li>模型结构是 [LINEAR -&gt; RELU]  ×  (L-1) -&gt; LINEAR -&gt; SIGMOID . 也就是说，有L-1层的ReLU激活函数，还有一层sigmoid输出</li>
<li>w用随机初始化</li>
<li>b用0初始化</li>
<li>将$n^{[l]}$存入数组变量layer_dims中，代表第l层的n</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    layer_dims: 数组变量，代表每一层的维度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dict , 包含 &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span></span><br><span class="line"><span class="string">            Wl -- 权重矩阵，W1.shape =  (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">            bl -- 偏置向量，b1.shape =  (layer_dims[l], 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims) <span class="comment"># 网络的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment"># 初始化第l层参数</span></span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l-<span class="number">1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 验证</span></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], layer_dims[l-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 0.00319039 -0.0024937   0.01462108 -0.02060141 -0.00322417]
 [-0.00384054  0.01133769 -0.01099891 -0.00172428 -0.00877858]
 [ 0.00042214  0.00582815 -0.01100619  0.01144724  0.00901591]
 [ 0.00502494  0.00900856 -0.00683728 -0.0012289  -0.00935769]]
b1 = [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
W2 = [[-0.00267888  0.00530355 -0.00691661 -0.00396754]
 [-0.00687173 -0.00845206 -0.00671246 -0.00012665]
 [-0.0111731   0.00234416  0.01659802  0.00742044]]
b2 = [[ 0.]
 [ 0.]
 [ 0.]]
</code></pre>
<h1 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h1><p>按照如下顺序实现：</p>
<ul>
<li>LINEAR</li>
<li>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</li>
<li>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model)</li>
</ul>
<h2 id="LINEAR"><a href="#LINEAR" class="headerlink" title="LINEAR"></a>LINEAR</h2><p>线性模型如下：</p>
<p>$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}$$</p>
<p>where $A^{[0]} = X$. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span>(<span class="params">A,W,b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现前向传播的线性部分</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    A -- 上一层的输出，也是本层的输入</span></span><br><span class="line"><span class="string">    W -- 权重矩阵 ， W.shape = (n_(l-1), n_l)</span></span><br><span class="line"><span class="string">    b -- 偏置向量 ， b.shape = (n_l , 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns : </span></span><br><span class="line"><span class="string">    Z -- 激活函数的输入，也叫&quot;pre-activation parameter&quot;</span></span><br><span class="line"><span class="string">    cache - dict，包含A,W,b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 线性部分</span></span><br><span class="line">    Z = np.dot(W,A) + b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 暂存</span></span><br><span class="line">    cache = (A, W, b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward_test_case</span>():</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.array([[-1.02387576, 1.12397796],</span></span><br><span class="line"><span class="string"> [-1.62328545, 0.64667545],</span></span><br><span class="line"><span class="string"> [-1.74314104, -0.59664964]])</span></span><br><span class="line"><span class="string">    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])</span></span><br><span class="line"><span class="string">    b = np.array([[1]])</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, W, b</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">A, W, b = linear_forward_test_case()</span><br><span class="line"></span><br><span class="line">Z, linear_cache = linear_forward(A, W, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Z = &quot;</span> + <span class="built_in">str</span>(Z))</span><br></pre></td></tr></table></figure>

<pre><code>Z = [[ 3.26295337 -1.23429987]]
</code></pre>
<p>​    </p>
<h2 id="LINEAR-gt-ACTIVATION"><a href="#LINEAR-gt-ACTIVATION" class="headerlink" title="LINEAR -&gt; ACTIVATION"></a>LINEAR -&gt; ACTIVATION</h2><p>本层的公式是：</p>
<p> $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$</p>
<p>其中,g()有两种选择：</p>
<p><strong>sigmod</strong> ： </p>
<p>$\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)}}$</p>
<p>sigmod的实现如下所示</p>
<p>输出:</p>
<ul>
<li>A</li>
<li>cache = z</li>
</ul>
<p><strong>ReLU</strong> :</p>
<p>$A = RELU(Z) = max(0, Z)$</p>
<p>ReLU的实现如下所示<br>输出：</p>
<ul>
<li>A</li>
<li>cache = Z</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">Z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the sigmoid activation in numpy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- numpy array of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class="line"><span class="string">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    A = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    cache = Z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">Z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the RELU function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- Output of the linear layer, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing &quot;A&quot; ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape)</span><br><span class="line"></span><br><span class="line">    cache = Z </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现LINEAR-&gt;ACTIVATION层</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span>(<span class="params">A_prev, W, b, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    LINEAR-&gt;ACTIVATION的实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    A_prev -- 上一层的输出</span></span><br><span class="line"><span class="string">    W -- 权重矩阵 ， W.shape = (n_l, n_(l-1))</span></span><br><span class="line"><span class="string">    b -- 偏置向量 ， b.shape = (n_l , 1)</span></span><br><span class="line"><span class="string">    activation -- 激活的名称，&quot;sigmoid&quot;或&quot;relu&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- 激活层输出，也叫post-activation value</span></span><br><span class="line"><span class="string">    cache  -- python dict, 包含 &quot;linear_cache&quot; 和 &quot;activation_cache&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 线性输出</span></span><br><span class="line">    Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 激活</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.array([[-1.02387576, 1.12397796],</span></span><br><span class="line"><span class="string"> [-1.62328545, 0.64667545],</span></span><br><span class="line"><span class="string"> [-1.74314104, -0.59664964]])</span></span><br><span class="line"><span class="string">    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])</span></span><br><span class="line"><span class="string">    b = 5</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    A_prev = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> A_prev, W, b</span><br><span class="line"></span><br><span class="line">A_prev, W, b = linear_activation_forward_test_case()</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;With sigmoid: A = &quot;</span> + <span class="built_in">str</span>(A))</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;With ReLU: A = &quot;</span> + <span class="built_in">str</span>(A))</span><br></pre></td></tr></table></figure>

<pre><code>With sigmoid: A = [[ 0.96890023  0.11013289]]
With ReLU: A = [[ 3.43896131  0.        ]]
</code></pre>
<h2 id="L-Model-Forward"><a href="#L-Model-Forward" class="headerlink" title="L-Model Forward"></a>L-Model Forward</h2><ul>
<li>用RELU，重复之前的linear_activation_forward，L-1次</li>
<li>最后输入SIGMOID 的linear_activation_forward</li>
</ul>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-16-51-52.png"> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现后向传播的[LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID步骤</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    X -- 数据集</span></span><br><span class="line"><span class="string">    parameters -- initialize_parameters_deep()的输出，是多层网络初始化后的参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    AL -- 最终输出</span></span><br><span class="line"><span class="string">    caches -- 每层的caches：</span></span><br><span class="line"><span class="string">            1 ~ L - 1层 的 linear_relu_forward()</span></span><br><span class="line"><span class="string">            L 层 的 linear_sigmoid_forward()</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    caches = []</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) / <span class="number">2</span></span><br><span class="line">    A = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1~ L-1 层，迭代</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)], parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)], <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># L 层     </span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(L)], parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(L)], <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.array([[-1.02387576, 1.12397796],</span></span><br><span class="line"><span class="string"> [-1.62328545, 0.64667545],</span></span><br><span class="line"><span class="string"> [-1.74314104, -0.59664964]])</span></span><br><span class="line"><span class="string">    parameters = &#123;&#x27;W1&#x27;: np.array([[ 1.62434536, -0.61175641, -0.52817175],</span></span><br><span class="line"><span class="string">        [-1.07296862,  0.86540763, -2.3015387 ]]),</span></span><br><span class="line"><span class="string"> &#x27;W2&#x27;: np.array([[ 1.74481176, -0.7612069 ]]),</span></span><br><span class="line"><span class="string"> &#x27;b1&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]]),</span></span><br><span class="line"><span class="string"> &#x27;b2&#x27;: np.array([[ 0.]])&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X = np.random.randn(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取X和初始化参数</span></span><br><span class="line">X, parameters = L_model_forward_test_case()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;X.shape = &quot;</span> + <span class="built_in">str</span>(X.shape) , <span class="string">&quot;，即&quot;</span>,X.shape[<span class="number">0</span>],<span class="string">&quot;行&quot;</span>,X.shape[<span class="number">1</span>],<span class="string">&quot;列&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">AL, caches = L_model_forward(X, parameters)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;AL = &quot;</span> + <span class="built_in">str</span>(AL))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Length of caches list = &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(caches)))</span><br></pre></td></tr></table></figure>

<pre><code>X.shape = (4L, 2L) ，即 4 行 2 列
AL = [[ 0.17007265  0.2524272 ]]
Length of caches list = 2
</code></pre>
<h1 id="计算误差"><a href="#计算误差" class="headerlink" title="计算误差"></a>计算误差</h1><p>误差公式如下：</p>
<p> $$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right)) \tag{7}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">AL, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现公式7的误差计算</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    AL -- 上一步计算出的结果</span></span><br><span class="line"><span class="string">    Y -- 实际的label</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns : </span></span><br><span class="line"><span class="string">    cost -- 交叉熵误差</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算</span></span><br><span class="line">    cost = -np.<span class="built_in">sum</span>(np.multiply(np.log(AL),Y) + np.multiply(np.log(<span class="number">1</span> - AL), <span class="number">1</span> - Y)) / m</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_test_case</span>():</span></span><br><span class="line">    Y = np.asarray([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">    aL = np.array([[<span class="number">.8</span>,<span class="number">.9</span>,<span class="number">0.4</span>]])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y, aL</span><br><span class="line"></span><br><span class="line">Y, AL = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;cost = &quot;</span> + <span class="built_in">str</span>(compute_cost(AL, Y)))</span><br></pre></td></tr></table></figure>

<pre><code>cost = 0.414931599615
</code></pre>
<p>​    </p>
<h1 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h1><p>后向传播是用来计算梯度的</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-17-11-51.png"> </p>
<p>紫色方块代表前向传播<br>红色方块代表后向传播</p>
<p>我们的目标是计算出$dw^{[l]}和db^{[l]}, l = 1,2,…L$，以便之后更新w和b。</p>
<p>为了计算$dw^{[1]}和db^{[1]}$，要使用如下链式法则：</p>
<p>$dw^{[1]}=\frac{dL}{dw^{[1]}}=\frac{dL}{dz^{[1]}} \times \frac{dz}{dw^{[1]}}=dz^{[1]}\times \frac{dz}{dw^{[1]}}$</p>
<p>$db^{[1]}=\frac{dL}{db^{[1]}} = \frac{dL}{dz^{[1]}} \times \frac{dz^{[1]}}{db^{[1]}} = dz^{[1]} \times \frac{dz^{[1]}}{db^{[1]}}$</p>
<p>因此我们首先要算出$dz^{[1]}$ : </p>
<p>$$\frac{dL(a^{[2]},y)}{dz^{[1]}}=\frac{dL(a^{[2]},y)}{da^{[2]}}\frac{da^{[2]}}{dz^{[2]}}\frac{dz^{[2]}}{da^{[1]}}\frac{da^{[1]}}{dz^{[1]}}$$</p>
<p>而要算出$dz^{[1]}$，由上公示可以看出，我们必须先计算$dz^{[2]}$等</p>
<p>因此此过程叫做<strong>后向传播</strong></p>
<p>总之，后向传播需要完成：</p>
<ul>
<li>LINEAR</li>
<li>LINEAR -&gt; ACTIVATION </li>
<li>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li>
</ul>
<h2 id="LINEAR-1"><a href="#LINEAR-1" class="headerlink" title="LINEAR"></a>LINEAR</h2><p>对于第l层来说，这一层的Linear部分是：$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$</p>
<p>假设此时你已经计算好了 $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$</p>
<p>你接下来想要得到$(dW^{[l]}, db^{[l]} dA^{[l-1]})$</p>
<p><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-08-20-17-43.png"> </p>
<p>我们可以通过如下的公式，通过$dZ^{[l]}$来计算出这三个东西$(dW^{[l]}, db^{[l]}, dA^{[l-1]})$:</p>
<p>$$ dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{8}$$<br>$$ db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{<a href="i">l</a>}\tag{9}$$<br>$$ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{10}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span>(<span class="params">dZ, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现后向传播的linear部分</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    dZ -- dL/dz</span></span><br><span class="line"><span class="string">    cache -- 三元组(A_prev, W, b) , 来自于当前层的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 上一层A的梯度</span></span><br><span class="line"><span class="string">    dW -- w的梯度</span></span><br><span class="line"><span class="string">    db -- b的梯度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算dw，db，dA_prev</span></span><br><span class="line">    dW = np.dot(dZ, A_prev.T) / m</span><br><span class="line">    db = np.<span class="built_in">sum</span>(dZ, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) / m </span><br><span class="line">    dA_prev = np.dot(W.T, dZ) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],</span></span><br><span class="line"><span class="string">       [-1.62328545,  0.64667545],</span></span><br><span class="line"><span class="string">       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    dZ = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    linear_cache = (A, W, b)</span><br><span class="line">    <span class="keyword">return</span> dZ, linear_cache </span><br><span class="line"></span><br><span class="line">dZ, linear_cache = linear_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA_prev = &quot;</span>+ <span class="built_in">str</span>(dA_prev))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW = &quot;</span> + <span class="built_in">str</span>(dW))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db = &quot;</span> + <span class="built_in">str</span>(db))</span><br></pre></td></tr></table></figure>

<pre><code>dA_prev = [[ 0.51822968 -0.19517421]
 [-0.40506361  0.15255393]
 [ 2.37496825 -0.89445391]]
dW = [[-0.10076895  1.40685096  1.64992505]]
db = [[ 0.50629448]]
</code></pre>
<h2 id="LINEAR-gt-ACTIVATION-1"><a href="#LINEAR-gt-ACTIVATION-1" class="headerlink" title="LINEAR -&gt; ACTIVATION"></a>LINEAR -&gt; ACTIVATION</h2><p>本节要加入后向传播中的activation部分：</p>
<p>假设$g(.)$是激活函数，</p>
<p>而下面给出的两个函数：<code>sigmoid_backward</code> 和 <code>relu_backward</code> 计算了$dL/dz$ :<br>$$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]}) \tag{11}$$. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span>(<span class="params">dA, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single RELU unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- &#x27;Z&#x27; where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    dZ = np.array(dA, copy=<span class="literal">True</span>) <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># When z &lt;= 0, you should set dz to 0 as well. </span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span>(<span class="params">dA, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- &#x27;Z&#x27; where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span>(<span class="params">dA, cache, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    后向传播的LINEAR-&gt;ACTIVATION实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    dA -- 当前层的输出A的导数</span></span><br><span class="line"><span class="string">    cache -- 二元组(linear_cache, activation_cache)，也就是之前前向传播计算时的cache</span></span><br><span class="line"><span class="string">    activation -- 本层的激活函数，是&quot;sigmoid&quot;或&quot;relu&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns :</span></span><br><span class="line"><span class="string">    dA_prev -- 上一层输出的导数</span></span><br><span class="line"><span class="string">    dW -- 本层W的导数</span></span><br><span class="line"><span class="string">    db -- 本层b的导数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    <span class="comment"># 计算dZ</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA,activation_cache)</span><br><span class="line">    <span class="comment"># 已知dZ, 计算dA_prev, dW, db</span></span><br><span class="line">    dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    dA = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    Z = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache = (A, W, b)</span><br><span class="line">    activation_cache = Z</span><br><span class="line">    linear_activation_cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA, linear_activation_cache</span><br><span class="line"></span><br><span class="line">AL, linear_activation_cache = linear_activation_backward_test_case()</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;sigmoid:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA_prev = &quot;</span>+ <span class="built_in">str</span>(dA_prev))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW = &quot;</span> + <span class="built_in">str</span>(dW))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db = &quot;</span> + <span class="built_in">str</span>(db) + <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;relu:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA_prev = &quot;</span>+ <span class="built_in">str</span>(dA_prev))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW = &quot;</span> + <span class="built_in">str</span>(dW))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db = &quot;</span> + <span class="built_in">str</span>(db))</span><br></pre></td></tr></table></figure>

<pre><code>sigmoid:
dA_prev = [[ 0.11017994  0.01105339]
 [ 0.09466817  0.00949723]
 [-0.05743092 -0.00576154]]
dW = [[ 0.10266786  0.09778551 -0.01968084]]
db = [[-0.05729622]]

(1L, 2L)
(1L, 2L)
relu:
dA_prev = [[ 0.44090989 -0.        ]
 [ 0.37883606 -0.        ]
 [-0.2298228   0.        ]]
dW = [[ 0.44513824  0.37371418 -0.10478989]]
db = [[-0.20837892]]
</code></pre>
<h2 id="L-Model-Backward"><a href="#L-Model-Backward" class="headerlink" title="L-Model Backward"></a>L-Model Backward</h2><p>接下来就该将后向传播应用在整个网络了。步骤如下：</p>
<ol>
<li>前向传播 - <code>L_model_forward()</code>,在每一层都存下了(X,W,b,z)</li>
<li>后向传播 - <code>L_model_backward()</code>，利用之前存的值，一层层向前计算导数</li>
</ol>
<p>后向传播计算导数的流程如下图所示：<br><img src="http://jiayi797.oss-cn-beijing.aliyuncs.com/2017-12-09-13-40-28.png"> </p>
<p><strong>初始化部分</strong></p>
<p>对于贯穿网络的后向传播来说，我们知道输出是$A^{[L]} = \sigma(Z^{[L]})$.<br>我们需要计算出 ：  <code>dAL</code> $= \frac{\partial \mathcal{L}}{\partial A^{[L]}}$.</p>
<p>为了完成这个目标，我们用如下公式实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># 相对于AL的成本衍生物</span></span><br></pre></td></tr></table></figure>
<p>这个公式计算了LINEAR-&gt;SIGMOID后向传播部分</p>
<p>接下来就该计算LINEAR-&gt;RELU后向传播部分了，这一部分需要存储每一步的dA, dW, db，用如下公式实现：<br>$$grads[“dW” + str(l)] = dW^{[l]}\tag{15} $$<br>例如，l=3，就将dw3 存储在<code>grads[&quot;dw3&quot;]</code>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span>(<span class="params">AL, Y, chache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID 的后向传播的实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    AL -- 概率向量，是前向传播L_model_forward()的输出</span></span><br><span class="line"><span class="string">    Y  -- 标签向量</span></span><br><span class="line"><span class="string">    caches -- caches列表：</span></span><br><span class="line"><span class="string">                     1 ~ L - 1 : relu的linear_activation_forward()的caches</span></span><br><span class="line"><span class="string">                     L         : sigmoid 的 linear_activation_forward()的caches</span></span><br><span class="line"><span class="string">                     </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    grads -- 梯度dict :</span></span><br><span class="line"><span class="string">        grads[&quot;dA&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">        grads[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">        grads[&quot;db&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches)</span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#后向传播的初始化</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算dAL,dWL,dbL</span></span><br><span class="line">    <span class="comment"># 第L层 - (SIGMOID -&gt; LINEAR)  - 的梯度</span></span><br><span class="line">    <span class="comment"># 输入 ：AL, Y, caches</span></span><br><span class="line">    <span class="comment"># 输出 ：grads[&quot;dAL&quot;], grads[&quot;dWL&quot;]</span></span><br><span class="line">    current_cache = caches[L - <span class="number">1</span>]</span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = linear_activation_backward(dAL,current_cache, <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算dAl,dWl,dbl , l = 1~L-1</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L-<span class="number">1</span>)):</span><br><span class="line">        <span class="comment"># 第l层：(RELU -&gt; LINEAR) 的梯度</span></span><br><span class="line">        <span class="comment"># 输入 ： &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. </span></span><br><span class="line">        <span class="comment"># 输出: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">2</span>)], current_cache, <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = db_temp</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    X = np.random.rand(3,2)</span></span><br><span class="line"><span class="string">    Y = np.array([[1, 1]])</span></span><br><span class="line"><span class="string">    parameters = &#123;&#x27;W1&#x27;: np.array([[ 1.78862847,  0.43650985,  0.09649747]]), &#x27;b1&#x27;: np.array([[ 0.]])&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],</span></span><br><span class="line"><span class="string">           [ 0.02738759,  0.67046751],</span></span><br><span class="line"><span class="string">           [ 0.4173048 ,  0.55868983]]),</span></span><br><span class="line"><span class="string">    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),</span></span><br><span class="line"><span class="string">    np.array([[ 0.]])),</span></span><br><span class="line"><span class="string">   np.array([[ 0.41791293,  1.91720367]]))])</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    AL = np.random.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    Y = np.array([[<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    A1 = np.random.randn(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    Z1 = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache_activation_1 = ((A1, W1, b1), Z1)</span><br><span class="line"></span><br><span class="line">    A2 = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    Z2 = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache_activation_2 = ( (A2, W2, b2), Z2)</span><br><span class="line"></span><br><span class="line">    caches = (linear_cache_activation_1, linear_cache_activation_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, Y, caches</span><br><span class="line"></span><br><span class="line">AL, Y_assess, caches = L_model_backward_test_case()</span><br><span class="line">grads = L_model_backward(AL, Y_assess, caches)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dW1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;db1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;dA1 = &quot;</span>+ <span class="built_in">str</span>(grads[<span class="string">&quot;dA1&quot;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>(3L, 2L)
(3L, 2L)
dW1 = [[ 0.41010002  0.07807203  0.13798444  0.10502167]
 [ 0.          0.          0.          0.        ]
 [ 0.05283652  0.01005865  0.01777766  0.0135308 ]]
db1 = [[-0.22007063]
 [ 0.        ]
 [-0.02835349]]
dA1 = [[ 0.          0.52257901]
 [ 0.         -0.3269206 ]
 [ 0.         -0.32070404]
 [ 0.         -0.74079187]]
</code></pre>
<h1 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h1><p>在这一部分我们用以上模型更新参数：</p>
<p>$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{16}$$<br>$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{17}$$</p>
<p>其中，$\alpha$是学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span>(<span class="params">parameters, grads, learning_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用梯度下降更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    parameters -- python dict，里面包含一些参数</span></span><br><span class="line"><span class="string">    grads -- python dict, 包含梯度</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dict, 包含更新后的参数：</span></span><br><span class="line"><span class="string">                  parameters[&quot;W&quot; + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters[&quot;b&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    L = <span class="built_in">len</span>(parameters) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] = parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] = parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_test_case</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    parameters = &#123;&#x27;W1&#x27;: np.array([[ 1.78862847,  0.43650985,  0.09649747],</span></span><br><span class="line"><span class="string">        [-1.8634927 , -0.2773882 , -0.35475898],</span></span><br><span class="line"><span class="string">        [-0.08274148, -0.62700068, -0.04381817],</span></span><br><span class="line"><span class="string">        [-0.47721803, -1.31386475,  0.88462238]]),</span></span><br><span class="line"><span class="string"> &#x27;W2&#x27;: np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],</span></span><br><span class="line"><span class="string">        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],</span></span><br><span class="line"><span class="string">        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),</span></span><br><span class="line"><span class="string"> &#x27;W3&#x27;: np.array([[-1.02378514, -0.7129932 ,  0.62524497],</span></span><br><span class="line"><span class="string">        [-0.16051336, -0.76883635, -0.23003072]]),</span></span><br><span class="line"><span class="string"> &#x27;b1&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.],</span></span><br><span class="line"><span class="string">        [ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]]),</span></span><br><span class="line"><span class="string"> &#x27;b2&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]]),</span></span><br><span class="line"><span class="string"> &#x27;b3&#x27;: np.array([[ 0.],</span></span><br><span class="line"><span class="string">        [ 0.]])&#125;</span></span><br><span class="line"><span class="string">    grads = &#123;&#x27;dW1&#x27;: np.array([[ 0.63070583,  0.66482653,  0.18308507],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ]]),</span></span><br><span class="line"><span class="string"> &#x27;dW2&#x27;: np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ,  0.        ],</span></span><br><span class="line"><span class="string">        [ 0.        ,  0.        ,  0.        ,  0.        ]]),</span></span><br><span class="line"><span class="string"> &#x27;dW3&#x27;: np.array([[-1.40260776,  0.        ,  0.        ]]),</span></span><br><span class="line"><span class="string"> &#x27;da1&#x27;: np.array([[ 0.70760786,  0.65063504],</span></span><br><span class="line"><span class="string">        [ 0.17268975,  0.15878569],</span></span><br><span class="line"><span class="string">        [ 0.03817582,  0.03510211]]),</span></span><br><span class="line"><span class="string"> &#x27;da2&#x27;: np.array([[ 0.39561478,  0.36376198],</span></span><br><span class="line"><span class="string">        [ 0.7674101 ,  0.70562233],</span></span><br><span class="line"><span class="string">        [ 0.0224596 ,  0.02065127],</span></span><br><span class="line"><span class="string">        [-0.18165561, -0.16702967]]),</span></span><br><span class="line"><span class="string"> &#x27;da3&#x27;: np.array([[ 0.44888991,  0.41274769],</span></span><br><span class="line"><span class="string">        [ 0.31261975,  0.28744927],</span></span><br><span class="line"><span class="string">        [-0.27414557, -0.25207283]]),</span></span><br><span class="line"><span class="string"> &#x27;db1&#x27;: 0.75937676204411464,</span></span><br><span class="line"><span class="string"> &#x27;db2&#x27;: 0.86163759922811056,</span></span><br><span class="line"><span class="string"> &#x27;db3&#x27;: -0.84161956022334572&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    dW1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    db1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    dW2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    db2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,</span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,</span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters, grads</span><br><span class="line"></span><br><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;W1 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;b1 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;W2 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;b2 = &quot;</span>+ <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W2 = [[-0.55569196  0.0354055   1.32964895]]
b2 = [[-0.84610769]]
</code></pre>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" rel="next" title="深度学习算法-几种常见的卷积网络">
                <i class="fa fa-chevron-left"></i> 深度学习算法-几种常见的卷积网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-1-4-2-%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E7%8C%AB/" rel="prev" title="深度学习实践-1-4-2-用神经网络识别猫">
                深度学习实践-1-4-2-用神经网络识别猫 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="SOHUCS"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="" />
          <p class="site-author-name" itemprop="name"></p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">141</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E7%BA%B2"><span class="nav-text">大纲</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#import"><span class="nav-text">import</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%A4%E5%B1%82%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">两层模型的初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L%E5%B1%82%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">L层模型的初始化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">前向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LINEAR"><span class="nav-text">LINEAR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LINEAR-gt-ACTIVATION"><span class="nav-text">LINEAR -&gt; ACTIVATION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-Model-Forward"><span class="nav-text">L-Model Forward</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E8%AF%AF%E5%B7%AE"><span class="nav-text">计算误差</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-text">后向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LINEAR-1"><span class="nav-text">LINEAR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LINEAR-gt-ACTIVATION-1"><span class="nav-text">LINEAR -&gt; ACTIVATION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-Model-Backward"><span class="nav-text">L-Model Backward</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0"><span class="nav-text">更新参数</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jiayi797</span>
</div>



<div class="theme-info">
  <div class="powered-by">感谢hexo.Next</div>
  <span class="post-count">博客全站共字</span>
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人次
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  




  
    <script type="text/javascript">
    (function(){
      var appid = 'cytmgt7V8';
      var conf = 'f20a47bca89136fdb1ce79762c886a35';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){
        window.changyan.api.config({appid:appid,conf:conf})});
      }
    })();
    </script>
    <script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>
  



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (search_path.endsWith("json")) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

</body>
</html>
